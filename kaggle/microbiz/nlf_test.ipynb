{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "488c470a-999e-4a3e-aab8-2aae2919b262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f06046b-a946-462c-93b0-4ee79041d136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERCOT</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>43719.849616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERCOT</td>\n",
       "      <td>2021-01-01 01:00:00</td>\n",
       "      <td>43321.050347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ERCOT</td>\n",
       "      <td>2021-01-01 02:00:00</td>\n",
       "      <td>43063.067063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ERCOT</td>\n",
       "      <td>2021-01-01 03:00:00</td>\n",
       "      <td>43090.059203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ERCOT</td>\n",
       "      <td>2021-01-01 04:00:00</td>\n",
       "      <td>43486.590073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                  ds             y\n",
       "0     ERCOT 2021-01-01 00:00:00  43719.849616\n",
       "1     ERCOT 2021-01-01 01:00:00  43321.050347\n",
       "2     ERCOT 2021-01-01 02:00:00  43063.067063\n",
       "3     ERCOT 2021-01-01 03:00:00  43090.059203\n",
       "4     ERCOT 2021-01-01 04:00:00  43486.590073"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4848e161-d63f-4151-bcb6-d3df74a7d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "from neuralforecast.auto import AutoTFT\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae09a251-8c5e-4bc6-89ae-d06c8dfde5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 24\n",
    "models = [AutoTFT(h=horizon,\n",
    "                  loss=MAE(),\n",
    "                  config=None,\n",
    "                  num_samples=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2682543c-d84f-44e2-8d87-bd8c138df13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_tune pid=12025)\u001b[0m 2023-02-03 14:09:38.412535: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(train_tune pid=12025)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(train_tune pid=12025)\u001b[0m 2023-02-03 14:09:47.116699: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "\u001b[2m\u001b[36m(train_tune pid=12025)\u001b[0m 2023-02-03 14:09:47.116831: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "\u001b[2m\u001b[36m(train_tune pid=12025)\u001b[0m 2023-02-03 14:09:47.116840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s] \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.63e+04, v_num=0, train_loss_step=4.63e+4, train_loss_epoch=4.63e+4]        \n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.65e+04, v_num=0, train_loss_step=4.66e+4, train_loss_epoch=4.66e+4]        \n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.67e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]         \n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.69e+04, v_num=0, train_loss_step=4.65e+4, train_loss_epoch=4.65e+4]        \n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00, 51.69it/s, loss=4.7e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4]\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4]        \n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 49.20it/s, loss=4.7e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]\n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 51.88it/s, loss=4.71e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4]\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4]        \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 51.81it/s, loss=4.72e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.63e+4, train_loss_epoch=4.63e+4]        \n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.56e+4, train_loss_epoch=4.56e+4]         \n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00, 50.62it/s, loss=4.69e+04, v_num=0, train_loss_step=4.66e+4, train_loss_epoch=4.66e+4]\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.69e+04, v_num=0, train_loss_step=4.64e+4, train_loss_epoch=4.64e+4]        \n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 51.48it/s, loss=4.68e+04, v_num=0, train_loss_step=4.58e+4, train_loss_epoch=4.58e+4]\n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 47.61it/s, loss=4.66e+04, v_num=0, train_loss_step=4.43e+4, train_loss_epoch=4.43e+4]\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.64e+04, v_num=0, train_loss_step=4.54e+4, train_loss_epoch=4.54e+4]        \n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00, 51.35it/s, loss=4.62e+04, v_num=0, train_loss_step=4.61e+4, train_loss_epoch=4.61e+4]\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.6e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]         \n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.58e+04, v_num=0, train_loss_step=4.6e+4, train_loss_epoch=4.6e+4]          \n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 51.91it/s, loss=4.58e+04, v_num=0, train_loss_step=4.6e+4, train_loss_epoch=4.6e+4]\n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 51.01it/s, loss=4.59e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4]\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.57e+04, v_num=0, train_loss_step=4.5e+4, train_loss_epoch=4.5e+4]          \n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 51.28it/s, loss=4.54e+04, v_num=0, train_loss_step=4.43e+4, train_loss_epoch=4.43e+4]\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 52.51it/s, loss=4.53e+04, v_num=0, train_loss_step=4.52e+4, train_loss_epoch=4.52e+4]\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 24.04it/s, loss=4.53e+04, v_num=0, train_loss_step=4.34e+4, train_loss_epoch=4.52e+4]\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 23.63it/s, loss=4.53e+04, v_num=0, train_loss_step=4.34e+4, train_loss_epoch=4.34e+4]\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.53e+04, v_num=0, train_loss_step=4.34e+4, train_loss_epoch=4.34e+4]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.5e+04, v_num=0, train_loss_step=4.38e+4, train_loss_epoch=4.38e+4]         \n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 48.58it/s, loss=4.48e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.31e+4]\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.45e+04, v_num=0, train_loss_step=4.28e+4, train_loss_epoch=4.28e+4]        \n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.42e+04, v_num=0, train_loss_step=4.33e+4, train_loss_epoch=4.33e+4]        \n",
      "Epoch 68: 100%|██████████| 1/1 [00:00<00:00, 47.92it/s, loss=4.39e+04, v_num=0, train_loss_step=4.24e+4, train_loss_epoch=4.24e+4]\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.34e+04, v_num=0, train_loss_step=4.18e+4, train_loss_epoch=4.18e+4]        \n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.3e+04, v_num=0, train_loss_step=4.08e+4, train_loss_epoch=4.08e+4]         \n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00, 35.74it/s, loss=4.27e+04, v_num=0, train_loss_step=4.11e+4, train_loss_epoch=4.11e+4]\n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.22e+04, v_num=0, train_loss_step=4.06e+4, train_loss_epoch=4.06e+4]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.19e+04, v_num=0, train_loss_step=3.99e+4, train_loss_epoch=3.99e+4]        \n",
      "Epoch 82: 100%|██████████| 1/1 [00:00<00:00, 51.86it/s, loss=4.16e+04, v_num=0, train_loss_step=3.98e+4, train_loss_epoch=3.98e+4]\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.11e+04, v_num=0, train_loss_step=4.01e+4, train_loss_epoch=4.01e+4]        \n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.07e+04, v_num=0, train_loss_step=3.88e+4, train_loss_epoch=3.88e+4]        \n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.03e+04, v_num=0, train_loss_step=3.75e+4, train_loss_epoch=3.75e+4]        \n",
      "Epoch 89: 100%|██████████| 1/1 [00:00<00:00, 49.02it/s, loss=4.03e+04, v_num=0, train_loss_step=3.75e+4, train_loss_epoch=3.75e+4]\n",
      "Epoch 91: 100%|██████████| 1/1 [00:00<00:00, 49.07it/s, loss=3.99e+04, v_num=0, train_loss_step=3.73e+4, train_loss_epoch=3.73e+4]\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.92e+04, v_num=0, train_loss_step=3.65e+4, train_loss_epoch=3.65e+4]        \n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.86e+04, v_num=0, train_loss_step=3.49e+4, train_loss_epoch=3.49e+4]        \n",
      "Epoch 96: 100%|██████████| 1/1 [00:00<00:00, 49.41it/s, loss=3.86e+04, v_num=0, train_loss_step=3.49e+4, train_loss_epoch=3.49e+4]\n",
      "Epoch 98: 100%|██████████| 1/1 [00:00<00:00, 48.99it/s, loss=3.81e+04, v_num=0, train_loss_step=3.51e+4, train_loss_epoch=3.51e+4]\n",
      "Epoch 99:  50%|█████     | 1/2 [00:00<00:00, 22.27it/s, loss=3.76e+04, v_num=0, train_loss_step=3.49e+4, train_loss_epoch=3.49e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 2/2 [00:00<00:00, 37.13it/s, loss=3.76e+04, v_num=0, train_loss_step=3.49e+4, train_loss_epoch=3.49e+4, val_loss=3.18e+4]\n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 49.83it/s, loss=3.76e+04, v_num=0, train_loss_step=3.49e+4, train_loss_epoch=3.49e+4, val_loss=3.18e+4]\n",
      "Epoch 103:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.68e+04, v_num=0, train_loss_step=3.45e+4, train_loss_epoch=3.45e+4, val_loss=3.18e+4]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.62e+04, v_num=0, train_loss_step=3.41e+4, train_loss_epoch=3.41e+4, val_loss=3.18e+4]        \n",
      "Epoch 107: 100%|██████████| 1/1 [00:00<00:00, 51.92it/s, loss=3.57e+04, v_num=0, train_loss_step=3.31e+4, train_loss_epoch=3.31e+4, val_loss=3.18e+4]\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.48e+04, v_num=0, train_loss_step=3.2e+4, train_loss_epoch=3.2e+4, val_loss=3.18e+4]          \n",
      "Epoch 112: 100%|██████████| 1/1 [00:00<00:00, 51.43it/s, loss=3.42e+04, v_num=0, train_loss_step=3.19e+4, train_loss_epoch=3.19e+4, val_loss=3.18e+4]\n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 52.29it/s, loss=3.37e+04, v_num=0, train_loss_step=3.07e+4, train_loss_epoch=3.07e+4, val_loss=3.18e+4]\n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.29e+04, v_num=0, train_loss_step=3.02e+4, train_loss_epoch=3.02e+4, val_loss=3.18e+4]        \n",
      "Epoch 119:   0%|          | 0/1 [00:00<?, ?it/s, loss=3.22e+04, v_num=0, train_loss_step=2.76e+4, train_loss_epoch=2.76e+4, val_loss=3.18e+4]        \n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00, 37.39it/s, loss=3.22e+04, v_num=0, train_loss_step=2.76e+4, train_loss_epoch=2.76e+4, val_loss=3.18e+4]\n",
      "Epoch 121: 100%|██████████| 1/1 [00:00<00:00, 47.40it/s, loss=3.14e+04, v_num=0, train_loss_step=2.63e+4, train_loss_epoch=2.63e+4, val_loss=3.18e+4]\n",
      "Epoch 123: 100%|██████████| 1/1 [00:00<00:00, 48.26it/s, loss=3.07e+04, v_num=0, train_loss_step=2.72e+4, train_loss_epoch=2.72e+4, val_loss=3.18e+4]\n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, loss=2.95e+04, v_num=0, train_loss_step=24136.0, train_loss_epoch=24136.0, val_loss=3.18e+4]        \n",
      "Epoch 128: 100%|██████████| 1/1 [00:00<00:00, 51.57it/s, loss=2.88e+04, v_num=0, train_loss_step=2.51e+4, train_loss_epoch=2.51e+4, val_loss=3.18e+4]\n",
      "Epoch 130: 100%|██████████| 1/1 [00:00<00:00, 23.62it/s, loss=2.77e+04, v_num=0, train_loss_step=2.52e+4, train_loss_epoch=2.37e+4, val_loss=3.18e+4]\n",
      "Epoch 131:   0%|          | 0/1 [00:00<?, ?it/s, loss=2.77e+04, v_num=0, train_loss_step=2.52e+4, train_loss_epoch=2.52e+4, val_loss=3.18e+4]        \n",
      "Epoch 133:   0%|          | 0/1 [00:00<?, ?it/s, loss=2.68e+04, v_num=0, train_loss_step=2.33e+4, train_loss_epoch=2.33e+4, val_loss=3.18e+4]        \n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, loss=2.59e+04, v_num=0, train_loss_step=2.26e+4, train_loss_epoch=2.26e+4, val_loss=3.18e+4]        \n",
      "Epoch 135: 100%|██████████| 1/1 [00:00<00:00, 47.81it/s, loss=2.59e+04, v_num=0, train_loss_step=2.26e+4, train_loss_epoch=2.26e+4, val_loss=3.18e+4]\n",
      "Epoch 137: 100%|██████████| 1/1 [00:00<00:00, 50.81it/s, loss=2.5e+04, v_num=0, train_loss_step=2.19e+4, train_loss_epoch=2.19e+4, val_loss=3.18e+4] \n",
      "Epoch 140:   0%|          | 0/1 [00:00<?, ?it/s, loss=2.4e+04, v_num=0, train_loss_step=19809.5, train_loss_epoch=19809.5, val_loss=3.18e+4]         \n",
      "Epoch 142: 100%|██████████| 1/1 [00:00<00:00, 50.21it/s, loss=2.32e+04, v_num=0, train_loss_step=1.89e+4, train_loss_epoch=1.89e+4, val_loss=3.18e+4]\n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00, 51.16it/s, loss=2.23e+04, v_num=0, train_loss_step=1.77e+4, train_loss_epoch=1.77e+4, val_loss=3.18e+4]\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, loss=2.11e+04, v_num=0, train_loss_step=1.69e+4, train_loss_epoch=1.69e+4, val_loss=3.18e+4]        \n",
      "Epoch 149: 100%|██████████| 1/1 [00:00<00:00, 51.61it/s, loss=2.02e+04, v_num=0, train_loss_step=1.5e+4, train_loss_epoch=1.5e+4, val_loss=3.18e+4]  \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.9e+04, v_num=0, train_loss_step=1.57e+4, train_loss_epoch=1.57e+4, val_loss=3.18e+4]         \n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.82e+04, v_num=0, train_loss_step=1.47e+4, train_loss_epoch=1.47e+4, val_loss=3.18e+4]        \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 51.27it/s, loss=1.73e+04, v_num=0, train_loss_step=1.26e+4, train_loss_epoch=1.26e+4, val_loss=3.18e+4]\n",
      "Epoch 159:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.59e+04, v_num=0, train_loss_step=1.23e+4, train_loss_epoch=1.23e+4, val_loss=3.18e+4]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.51e+04, v_num=0, train_loss_step=1.17e+4, train_loss_epoch=1.17e+4, val_loss=3.18e+4]        \n",
      "Epoch 163: 100%|██████████| 1/1 [00:00<00:00, 48.58it/s, loss=1.42e+04, v_num=0, train_loss_step=9e+3, train_loss_epoch=9e+3, val_loss=3.18e+4]      \n",
      "Epoch 165: 100%|██████████| 1/1 [00:00<00:00, 50.77it/s, loss=1.34e+04, v_num=0, train_loss_step=1e+4, train_loss_epoch=1e+4, val_loss=3.18e+4]      \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.22e+04, v_num=0, train_loss_step=8.9e+3, train_loss_epoch=8.9e+3, val_loss=3.18e+4]          \n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 50.25it/s, loss=1.16e+04, v_num=0, train_loss_step=9.59e+3, train_loss_epoch=9.59e+3, val_loss=3.18e+4]\n",
      "Epoch 172: 100%|██████████| 1/1 [00:00<00:00, 50.27it/s, loss=1.09e+04, v_num=0, train_loss_step=9e+3, train_loss_epoch=9e+3, val_loss=3.18e+4]      \n",
      "Epoch 175:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.01e+04, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=3.18e+4]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, loss=9.78e+03, v_num=0, train_loss_step=8.9e+3, train_loss_epoch=8.9e+3, val_loss=3.18e+4]          \n",
      "Epoch 179: 100%|██████████| 1/1 [00:00<00:00, 48.95it/s, loss=9.33e+03, v_num=0, train_loss_step=7.66e+3, train_loss_epoch=7.66e+3, val_loss=3.18e+4]\n",
      "Epoch 181: 100%|██████████| 1/1 [00:00<00:00, 49.62it/s, loss=9.05e+03, v_num=0, train_loss_step=8.8e+3, train_loss_epoch=8.8e+3, val_loss=3.18e+4]  \n",
      "Epoch 184:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.85e+03, v_num=0, train_loss_step=7.77e+3, train_loss_epoch=7.77e+3, val_loss=3.18e+4]        \n",
      "Epoch 186: 100%|██████████| 1/1 [00:00<00:00, 51.84it/s, loss=8.71e+03, v_num=0, train_loss_step=8.59e+3, train_loss_epoch=8.59e+3, val_loss=3.18e+4]\n",
      "Epoch 189:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.55e+03, v_num=0, train_loss_step=8.66e+3, train_loss_epoch=8.66e+3, val_loss=3.18e+4]        \n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.51e+03, v_num=0, train_loss_step=8.2e+3, train_loss_epoch=8.2e+3, val_loss=3.18e+4]          \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.5e+03, v_num=0, train_loss_step=8.52e+3, train_loss_epoch=8.52e+3, val_loss=3.18e+4]        \n",
      "Epoch 195: 100%|██████████| 1/1 [00:00<00:00, 51.13it/s, loss=8.43e+03, v_num=0, train_loss_step=7.82e+3, train_loss_epoch=7.82e+3, val_loss=3.18e+4]\n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=3.18e+4]        \n",
      "Epoch 199:  50%|█████     | 1/2 [00:00<00:00, 23.16it/s, loss=8.43e+03, v_num=0, train_loss_step=8.4e+3, train_loss_epoch=8.52e+3, val_loss=3.18e+4] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 38.69it/s, loss=8.43e+03, v_num=0, train_loss_step=8.4e+3, train_loss_epoch=8.52e+3, val_loss=6.02e+3]\n",
      "Epoch 200:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.43e+03, v_num=0, train_loss_step=8.4e+3, train_loss_epoch=8.4e+3, val_loss=6.02e+3]         \n",
      "Epoch 202: 100%|██████████| 1/1 [00:00<00:00, 50.82it/s, loss=8.38e+03, v_num=0, train_loss_step=8.04e+3, train_loss_epoch=8.04e+3, val_loss=6.02e+3]\n",
      "Epoch 204: 100%|██████████| 1/1 [00:00<00:00, 49.23it/s, loss=8.39e+03, v_num=0, train_loss_step=8.32e+3, train_loss_epoch=8.32e+3, val_loss=6.02e+3]\n",
      "Epoch 206: 100%|██████████| 1/1 [00:00<00:00, 49.03it/s, loss=8.37e+03, v_num=0, train_loss_step=8.38e+3, train_loss_epoch=8.38e+3, val_loss=6.02e+3]\n",
      "Epoch 209:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.49e+3, train_loss_epoch=8.49e+3, val_loss=6.02e+3]        \n",
      "Epoch 211: 100%|██████████| 1/1 [00:00<00:00, 51.16it/s, loss=8.39e+03, v_num=0, train_loss_step=8.79e+3, train_loss_epoch=8.79e+3, val_loss=6.02e+3]\n",
      "Epoch 214:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.4e+03, v_num=0, train_loss_step=8.54e+3, train_loss_epoch=8.54e+3, val_loss=6.02e+3]         \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.4e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=6.02e+3]         \n",
      "Epoch 218: 100%|██████████| 1/1 [00:00<00:00, 50.70it/s, loss=8.39e+03, v_num=0, train_loss_step=8.46e+3, train_loss_epoch=8.46e+3, val_loss=6.02e+3]\n",
      "Epoch 220: 100%|██████████| 1/1 [00:00<00:00, 49.01it/s, loss=8.34e+03, v_num=0, train_loss_step=7.63e+3, train_loss_epoch=7.63e+3, val_loss=6.02e+3]\n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.41e+03, v_num=0, train_loss_step=8.87e+3, train_loss_epoch=8.87e+3, val_loss=6.02e+3]        \n",
      "Epoch 225:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.45e+03, v_num=0, train_loss_step=8.08e+3, train_loss_epoch=8.08e+3, val_loss=6.02e+3]        \n",
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00, 51.55it/s, loss=8.44e+03, v_num=0, train_loss_step=8.71e+3, train_loss_epoch=8.71e+3, val_loss=6.02e+3]\n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=7.74e+3, train_loss_epoch=7.74e+3, val_loss=6.02e+3]        \n",
      "Epoch 232: 100%|██████████| 1/1 [00:00<00:00, 49.16it/s, loss=8.37e+03, v_num=0, train_loss_step=8.6e+3, train_loss_epoch=8.6e+3, val_loss=6.02e+3]  \n",
      "Epoch 234: 100%|██████████| 1/1 [00:00<00:00, 47.82it/s, loss=8.33e+03, v_num=0, train_loss_step=8.28e+3, train_loss_epoch=8.28e+3, val_loss=6.02e+3]\n",
      "Epoch 236: 100%|██████████| 1/1 [00:00<00:00, 49.14it/s, loss=8.31e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=6.02e+3]\n",
      "Epoch 239:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.3e+03, v_num=0, train_loss_step=8.15e+3, train_loss_epoch=8.15e+3, val_loss=6.02e+3]         \n",
      "Epoch 241: 100%|██████████| 1/1 [00:00<00:00, 49.52it/s, loss=8.35e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=6.02e+3]\n",
      "Epoch 243: 100%|██████████| 1/1 [00:00<00:00, 23.87it/s, loss=8.23e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.18e+3, val_loss=6.02e+3]\n",
      "Epoch 243: 100%|██████████| 1/1 [00:00<00:00, 23.47it/s, loss=8.23e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=6.02e+3]\n",
      "Epoch 244:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.23e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=6.02e+3]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.27e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=6.02e+3]        \n",
      "Epoch 248: 100%|██████████| 1/1 [00:00<00:00, 52.65it/s, loss=8.26e+03, v_num=0, train_loss_step=8.79e+3, train_loss_epoch=8.79e+3, val_loss=6.02e+3]\n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.25e+3, train_loss_epoch=8.25e+3, val_loss=6.02e+3]        \n",
      "Epoch 253:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.33e+03, v_num=0, train_loss_step=8.41e+3, train_loss_epoch=8.41e+3, val_loss=6.02e+3]        \n",
      "Epoch 253: 100%|██████████| 1/1 [00:00<00:00, 51.58it/s, loss=8.33e+03, v_num=0, train_loss_step=8.41e+3, train_loss_epoch=8.41e+3, val_loss=6.02e+3]\n",
      "Epoch 255: 100%|██████████| 1/1 [00:00<00:00, 51.52it/s, loss=8.38e+03, v_num=0, train_loss_step=8.63e+3, train_loss_epoch=8.63e+3, val_loss=6.02e+3]\n",
      "Epoch 258:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.42e+03, v_num=0, train_loss_step=8.1e+3, train_loss_epoch=8.1e+3, val_loss=6.02e+3]          \n",
      "Epoch 260: 100%|██████████| 1/1 [00:00<00:00, 51.61it/s, loss=8.41e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=6.02e+3]\n",
      "Epoch 262: 100%|██████████| 1/1 [00:00<00:00, 51.53it/s, loss=8.47e+03, v_num=0, train_loss_step=9.09e+3, train_loss_epoch=9.09e+3, val_loss=6.02e+3]\n",
      "Epoch 265:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.54e+03, v_num=0, train_loss_step=8.73e+3, train_loss_epoch=8.73e+3, val_loss=6.02e+3]        \n",
      "Epoch 267: 100%|██████████| 1/1 [00:00<00:00, 49.49it/s, loss=8.55e+03, v_num=0, train_loss_step=8.56e+3, train_loss_epoch=8.56e+3, val_loss=6.02e+3]\n",
      "Epoch 270:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.5e+03, v_num=0, train_loss_step=8.32e+3, train_loss_epoch=8.32e+3, val_loss=6.02e+3]         \n",
      "Epoch 272:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.48e+03, v_num=0, train_loss_step=7.97e+3, train_loss_epoch=7.97e+3, val_loss=6.02e+3]        \n",
      "Epoch 274: 100%|██████████| 1/1 [00:00<00:00, 52.17it/s, loss=8.47e+03, v_num=0, train_loss_step=8.24e+3, train_loss_epoch=8.24e+3, val_loss=6.02e+3]\n",
      "Epoch 277:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.44e+03, v_num=0, train_loss_step=8.76e+3, train_loss_epoch=8.76e+3, val_loss=6.02e+3]        \n",
      "Epoch 279: 100%|██████████| 1/1 [00:00<00:00, 50.93it/s, loss=8.48e+03, v_num=0, train_loss_step=8.39e+3, train_loss_epoch=8.39e+3, val_loss=6.02e+3]\n",
      "Epoch 281: 100%|██████████| 1/1 [00:00<00:00, 51.21it/s, loss=8.48e+03, v_num=0, train_loss_step=8.64e+3, train_loss_epoch=8.64e+3, val_loss=6.02e+3]\n",
      "Epoch 284:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.24e+3, train_loss_epoch=8.24e+3, val_loss=6.02e+3]        \n",
      "Epoch 286: 100%|██████████| 1/1 [00:00<00:00, 51.66it/s, loss=8.36e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=6.02e+3]\n",
      "Epoch 288: 100%|██████████| 1/1 [00:00<00:00, 50.03it/s, loss=8.34e+03, v_num=0, train_loss_step=8.14e+3, train_loss_epoch=8.14e+3, val_loss=6.02e+3]\n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=6.02e+3]        \n",
      "Epoch 293:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=7.8e+3, train_loss_epoch=7.8e+3, val_loss=6.02e+3]          \n",
      "Epoch 295: 100%|██████████| 1/1 [00:00<00:00, 49.06it/s, loss=8.3e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=6.02e+3] \n",
      "Epoch 298:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.2e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=6.02e+3]         \n",
      "Epoch 299:  50%|█████     | 1/2 [00:00<00:00, 23.85it/s, loss=8.18e+03, v_num=0, train_loss_step=7.83e+3, train_loss_epoch=8.55e+3, val_loss=6.02e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 39.60it/s, loss=8.18e+03, v_num=0, train_loss_step=7.83e+3, train_loss_epoch=8.55e+3, val_loss=5.89e+3]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.18e+03, v_num=0, train_loss_step=7.83e+3, train_loss_epoch=7.83e+3, val_loss=5.89e+3]        \n",
      "Epoch 302: 100%|██████████| 1/1 [00:00<00:00, 49.31it/s, loss=8.17e+03, v_num=0, train_loss_step=8.4e+3, train_loss_epoch=8.4e+3, val_loss=5.89e+3]  \n",
      "Epoch 304: 100%|██████████| 1/1 [00:00<00:00, 50.52it/s, loss=8.14e+03, v_num=0, train_loss_step=7.74e+3, train_loss_epoch=7.74e+3, val_loss=5.89e+3]\n",
      "Epoch 307:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.12e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.89e+3]        \n",
      "Epoch 309: 100%|██████████| 1/1 [00:00<00:00, 50.61it/s, loss=8.15e+03, v_num=0, train_loss_step=8.09e+3, train_loss_epoch=8.09e+3, val_loss=5.89e+3]\n",
      "Epoch 311: 100%|██████████| 1/1 [00:00<00:00, 24.14it/s, loss=8.11e+03, v_num=0, train_loss_step=8.32e+3, train_loss_epoch=7.84e+3, val_loss=5.89e+3]\n",
      "Epoch 311: 100%|██████████| 1/1 [00:00<00:00, 23.81it/s, loss=8.11e+03, v_num=0, train_loss_step=8.32e+3, train_loss_epoch=8.32e+3, val_loss=5.89e+3]\n",
      "Epoch 312:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.11e+03, v_num=0, train_loss_step=8.32e+3, train_loss_epoch=8.32e+3, val_loss=5.89e+3]        \n",
      "Epoch 314:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.13e+03, v_num=0, train_loss_step=7.99e+3, train_loss_epoch=7.99e+3, val_loss=5.89e+3]        \n",
      "Epoch 316: 100%|██████████| 1/1 [00:00<00:00, 51.56it/s, loss=8.08e+03, v_num=0, train_loss_step=8e+3, train_loss_epoch=8e+3, val_loss=5.89e+3]      \n",
      "Epoch 319:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.1e+03, v_num=0, train_loss_step=8.02e+3, train_loss_epoch=8.02e+3, val_loss=5.89e+3]         \n",
      "Epoch 321:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.17e+03, v_num=0, train_loss_step=8.57e+3, train_loss_epoch=8.57e+3, val_loss=5.89e+3]        \n",
      "Epoch 323: 100%|██████████| 1/1 [00:00<00:00, 50.90it/s, loss=8.18e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=5.89e+3]\n",
      "Epoch 326:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.24e+03, v_num=0, train_loss_step=8.66e+3, train_loss_epoch=8.66e+3, val_loss=5.89e+3]        \n",
      "Epoch 328:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.2e+03, v_num=0, train_loss_step=7.8e+3, train_loss_epoch=7.8e+3, val_loss=5.89e+3]           \n",
      "Epoch 330: 100%|██████████| 1/1 [00:00<00:00, 51.32it/s, loss=8.3e+03, v_num=0, train_loss_step=8.88e+3, train_loss_epoch=8.88e+3, val_loss=5.89e+3] \n",
      "Epoch 333:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.67e+3, train_loss_epoch=8.67e+3, val_loss=5.89e+3]        \n",
      "Epoch 335: 100%|██████████| 1/1 [00:00<00:00, 52.59it/s, loss=8.42e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=5.89e+3]\n",
      "Epoch 337: 100%|██████████| 1/1 [00:00<00:00, 50.69it/s, loss=8.43e+03, v_num=0, train_loss_step=8.08e+3, train_loss_epoch=8.08e+3, val_loss=5.89e+3]\n",
      "Epoch 340:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.46e+03, v_num=0, train_loss_step=8.55e+3, train_loss_epoch=8.55e+3, val_loss=5.89e+3]        \n",
      "Epoch 342: 100%|██████████| 1/1 [00:00<00:00, 51.38it/s, loss=8.39e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=5.89e+3]\n",
      "Epoch 344: 100%|██████████| 1/1 [00:00<00:00, 51.12it/s, loss=8.44e+03, v_num=0, train_loss_step=8.54e+3, train_loss_epoch=8.54e+3, val_loss=5.89e+3]\n",
      "Epoch 347:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.43e+03, v_num=0, train_loss_step=8.72e+3, train_loss_epoch=8.72e+3, val_loss=5.89e+3]        \n",
      "Epoch 349:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.43e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=5.89e+3]        \n",
      "Epoch 351: 100%|██████████| 1/1 [00:00<00:00, 49.52it/s, loss=8.39e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.89e+3]\n",
      "Epoch 354:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.62e+3, train_loss_epoch=8.62e+3, val_loss=5.89e+3]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.3e+03, v_num=0, train_loss_step=8.23e+3, train_loss_epoch=8.23e+3, val_loss=5.89e+3]         \n",
      "Epoch 356: 100%|██████████| 1/1 [00:00<00:00, 51.62it/s, loss=8.3e+03, v_num=0, train_loss_step=8.23e+3, train_loss_epoch=8.23e+3, val_loss=5.89e+3]\n",
      "Epoch 358: 100%|██████████| 1/1 [00:00<00:00, 49.44it/s, loss=8.33e+03, v_num=0, train_loss_step=8.07e+3, train_loss_epoch=8.07e+3, val_loss=5.89e+3]\n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.26e+3, train_loss_epoch=8.26e+3, val_loss=5.89e+3]        \n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=8.13e+3, train_loss_epoch=8.13e+3, val_loss=5.89e+3]        \n",
      "Epoch 365: 100%|██████████| 1/1 [00:00<00:00, 51.22it/s, loss=8.27e+03, v_num=0, train_loss_step=8.07e+3, train_loss_epoch=8.07e+3, val_loss=5.89e+3]\n",
      "Epoch 367: 100%|██████████| 1/1 [00:00<00:00, 51.24it/s, loss=8.27e+03, v_num=0, train_loss_step=8.13e+3, train_loss_epoch=8.13e+3, val_loss=5.89e+3]\n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=8.69e+3, train_loss_epoch=8.69e+3, val_loss=5.89e+3]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.27e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=5.89e+3]        \n",
      "Epoch 374: 100%|██████████| 1/1 [00:00<00:00, 50.02it/s, loss=8.25e+03, v_num=0, train_loss_step=8.67e+3, train_loss_epoch=8.67e+3, val_loss=5.89e+3]\n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.27e+03, v_num=0, train_loss_step=8.54e+3, train_loss_epoch=8.54e+3, val_loss=5.89e+3]        \n",
      "Epoch 379:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=8.28e+3, train_loss_epoch=8.28e+3, val_loss=5.89e+3]        \n",
      "Epoch 381: 100%|██████████| 1/1 [00:00<00:00, 49.15it/s, loss=8.26e+03, v_num=0, train_loss_step=7.6e+3, train_loss_epoch=7.6e+3, val_loss=5.89e+3]  \n",
      "Epoch 383: 100%|██████████| 1/1 [00:00<00:00, 50.75it/s, loss=8.24e+03, v_num=0, train_loss_step=7.85e+3, train_loss_epoch=7.85e+3, val_loss=5.89e+3]\n",
      "Epoch 386:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=9.14e+3, train_loss_epoch=9.14e+3, val_loss=5.89e+3]        \n",
      "Epoch 388: 100%|██████████| 1/1 [00:00<00:00, 48.93it/s, loss=8.32e+03, v_num=0, train_loss_step=9.03e+3, train_loss_epoch=9.03e+3, val_loss=5.89e+3]\n",
      "Epoch 390: 100%|██████████| 1/1 [00:00<00:00, 48.44it/s, loss=8.29e+03, v_num=0, train_loss_step=7.59e+3, train_loss_epoch=7.59e+3, val_loss=5.89e+3]\n",
      "Epoch 392: 100%|██████████| 1/1 [00:00<00:00, 23.07it/s, loss=8.31e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.89e+3]\n",
      "Epoch 393:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.89e+3]        \n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.18e+3, train_loss_epoch=8.18e+3, val_loss=5.89e+3]        \n",
      "Epoch 397: 100%|██████████| 1/1 [00:00<00:00, 51.41it/s, loss=8.28e+03, v_num=0, train_loss_step=8.25e+3, train_loss_epoch=8.25e+3, val_loss=5.89e+3]\n",
      "Epoch 399:  50%|█████     | 1/2 [00:00<00:00, 23.41it/s, loss=8.28e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.53e+3, val_loss=5.89e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(train_tune pid=12025)\u001b[0m \n",
      "Epoch 399: 100%|██████████| 2/2 [00:00<00:00, 38.41it/s, loss=8.28e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.53e+3, val_loss=5.89e+3]\n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=5.89e+3]        \n",
      "Epoch 404: 100%|██████████| 1/1 [00:00<00:00, 48.47it/s, loss=8.45e+03, v_num=0, train_loss_step=8.87e+3, train_loss_epoch=8.87e+3, val_loss=5.89e+3]\n",
      "Epoch 406: 100%|██████████| 1/1 [00:00<00:00, 51.87it/s, loss=8.42e+03, v_num=0, train_loss_step=8.57e+3, train_loss_epoch=8.57e+3, val_loss=5.89e+3]\n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.37e+03, v_num=0, train_loss_step=7.85e+3, train_loss_epoch=7.85e+3, val_loss=5.89e+3]        \n",
      "Epoch 411: 100%|██████████| 1/1 [00:00<00:00, 48.79it/s, loss=8.39e+03, v_num=0, train_loss_step=8.34e+3, train_loss_epoch=8.34e+3, val_loss=5.89e+3]\n",
      "Epoch 413: 100%|██████████| 1/1 [00:00<00:00, 51.25it/s, loss=8.36e+03, v_num=0, train_loss_step=8.23e+3, train_loss_epoch=8.23e+3, val_loss=5.89e+3]\n",
      "Epoch 413: 100%|██████████| 1/1 [00:00<00:00, 23.69it/s, loss=8.35e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.89e+3]\n",
      "Epoch 414:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.35e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.89e+3]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.35e+03, v_num=0, train_loss_step=8.21e+3, train_loss_epoch=8.21e+3, val_loss=5.89e+3]        \n",
      "Epoch 418: 100%|██████████| 1/1 [00:00<00:00, 51.84it/s, loss=8.34e+03, v_num=0, train_loss_step=8.21e+3, train_loss_epoch=8.21e+3, val_loss=5.89e+3]\n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.3e+03, v_num=0, train_loss_step=8.11e+3, train_loss_epoch=8.11e+3, val_loss=5.89e+3]         \n",
      "Epoch 423: 100%|██████████| 1/1 [00:00<00:00, 49.28it/s, loss=8.27e+03, v_num=0, train_loss_step=8.02e+3, train_loss_epoch=8.02e+3, val_loss=5.89e+3]\n",
      "Epoch 425: 100%|██████████| 1/1 [00:00<00:00, 51.26it/s, loss=8.26e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=5.89e+3]\n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.84e+3, train_loss_epoch=8.84e+3, val_loss=5.89e+3]        \n",
      "Epoch 430:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.06e+3, train_loss_epoch=8.06e+3, val_loss=5.89e+3]        \n",
      "Epoch 432: 100%|██████████| 1/1 [00:00<00:00, 46.18it/s, loss=8.36e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=5.89e+3]\n",
      "Epoch 435:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.38e+03, v_num=0, train_loss_step=8.49e+3, train_loss_epoch=8.49e+3, val_loss=5.89e+3]        \n",
      "Epoch 437:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.43e+03, v_num=0, train_loss_step=8.74e+3, train_loss_epoch=8.74e+3, val_loss=5.89e+3]        \n",
      "Epoch 439: 100%|██████████| 1/1 [00:00<00:00, 49.34it/s, loss=8.49e+03, v_num=0, train_loss_step=8.85e+3, train_loss_epoch=8.85e+3, val_loss=5.89e+3]\n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.52e+03, v_num=0, train_loss_step=8.38e+3, train_loss_epoch=8.38e+3, val_loss=5.89e+3]        \n",
      "Epoch 444: 100%|██████████| 1/1 [00:00<00:00, 50.14it/s, loss=8.51e+03, v_num=0, train_loss_step=8.11e+3, train_loss_epoch=8.11e+3, val_loss=5.89e+3]\n",
      "Epoch 446: 100%|██████████| 1/1 [00:00<00:00, 50.67it/s, loss=8.49e+03, v_num=0, train_loss_step=8.15e+3, train_loss_epoch=8.15e+3, val_loss=5.89e+3]\n",
      "Epoch 449:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.45e+03, v_num=0, train_loss_step=8.38e+3, train_loss_epoch=8.38e+3, val_loss=5.89e+3]        \n",
      "Epoch 451: 100%|██████████| 1/1 [00:00<00:00, 50.33it/s, loss=8.43e+03, v_num=0, train_loss_step=8.26e+3, train_loss_epoch=8.26e+3, val_loss=5.89e+3]\n",
      "Epoch 453: 100%|██████████| 1/1 [00:00<00:00, 50.24it/s, loss=8.39e+03, v_num=0, train_loss_step=7.79e+3, train_loss_epoch=7.79e+3, val_loss=5.89e+3]\n",
      "Epoch 456:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.11e+3, train_loss_epoch=8.11e+3, val_loss=5.89e+3]        \n",
      "Epoch 458: 100%|██████████| 1/1 [00:00<00:00, 49.88it/s, loss=8.38e+03, v_num=0, train_loss_step=8.29e+3, train_loss_epoch=8.29e+3, val_loss=5.89e+3]\n",
      "Epoch 460: 100%|██████████| 1/1 [00:00<00:00, 48.20it/s, loss=8.31e+03, v_num=0, train_loss_step=7.77e+3, train_loss_epoch=7.77e+3, val_loss=5.89e+3]\n",
      "Epoch 460: 100%|██████████| 1/1 [00:00<00:00, 22.86it/s, loss=8.29e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.89e+3]\n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.89e+3]        \n",
      "Epoch 463:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.88e+3, train_loss_epoch=8.88e+3, val_loss=5.89e+3]        \n",
      "Epoch 465: 100%|██████████| 1/1 [00:00<00:00, 50.41it/s, loss=8.33e+03, v_num=0, train_loss_step=8.72e+3, train_loss_epoch=8.72e+3, val_loss=5.89e+3]\n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.14e+3, train_loss_epoch=8.14e+3, val_loss=5.89e+3]        \n",
      "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.22e+3, train_loss_epoch=8.22e+3, val_loss=5.89e+3]        \n",
      "Epoch 472: 100%|██████████| 1/1 [00:00<00:00, 50.20it/s, loss=8.34e+03, v_num=0, train_loss_step=8.39e+3, train_loss_epoch=8.39e+3, val_loss=5.89e+3]\n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8087.25, train_loss_epoch=8087.25, val_loss=5.89e+3]        \n",
      "Epoch 477:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=5.89e+3]        \n",
      "Epoch 479: 100%|██████████| 1/1 [00:00<00:00, 47.63it/s, loss=8.29e+03, v_num=0, train_loss_step=7.94e+3, train_loss_epoch=7.94e+3, val_loss=5.89e+3]\n",
      "Epoch 482:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.49e+3, train_loss_epoch=8.49e+3, val_loss=5.89e+3]        \n",
      "Epoch 484: 100%|██████████| 1/1 [00:00<00:00, 51.10it/s, loss=8.22e+03, v_num=0, train_loss_step=7.74e+3, train_loss_epoch=7.74e+3, val_loss=5.89e+3]\n",
      "Epoch 486: 100%|██████████| 1/1 [00:00<00:00, 50.46it/s, loss=8.19e+03, v_num=0, train_loss_step=8.38e+3, train_loss_epoch=8.38e+3, val_loss=5.89e+3]\n",
      "Epoch 489:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.2e+03, v_num=0, train_loss_step=8.18e+3, train_loss_epoch=8.18e+3, val_loss=5.89e+3]         \n",
      "Epoch 491: 100%|██████████| 1/1 [00:00<00:00, 49.67it/s, loss=8.21e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.89e+3]\n",
      "Epoch 493: 100%|██████████| 1/1 [00:00<00:00, 49.33it/s, loss=8.25e+03, v_num=0, train_loss_step=8.56e+3, train_loss_epoch=8.56e+3, val_loss=5.89e+3]\n",
      "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.63e+3, train_loss_epoch=8.63e+3, val_loss=5.89e+3]        \n",
      "Epoch 498:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.49e+3, train_loss_epoch=8.49e+3, val_loss=5.89e+3]        \n",
      "Epoch 499:  50%|█████     | 1/2 [00:00<00:00, 21.81it/s, loss=8.25e+03, v_num=0, train_loss_step=8.13e+3, train_loss_epoch=7.95e+3, val_loss=5.89e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00, 36.05it/s, loss=8.25e+03, v_num=0, train_loss_step=8.13e+3, train_loss_epoch=7.95e+3, val_loss=5.93e+3]\n",
      "Epoch 500:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=8.13e+3, train_loss_epoch=8.13e+3, val_loss=5.93e+3]        \n",
      "Epoch 502:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.24e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.93e+3]        \n",
      "Epoch 502: 100%|██████████| 1/1 [00:00<00:00, 47.51it/s, loss=8.24e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.93e+3]\n",
      "Epoch 504: 100%|██████████| 1/1 [00:00<00:00, 49.37it/s, loss=8.32e+03, v_num=0, train_loss_step=8.49e+3, train_loss_epoch=8.49e+3, val_loss=5.93e+3]\n",
      "Epoch 507:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.35e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.93e+3]        \n",
      "Epoch 509:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.26e+3, train_loss_epoch=8.26e+3, val_loss=5.93e+3]        \n",
      "Epoch 511: 100%|██████████| 1/1 [00:00<00:00, 48.96it/s, loss=8.35e+03, v_num=0, train_loss_step=8.61e+3, train_loss_epoch=8.61e+3, val_loss=5.93e+3]\n",
      "Epoch 513: 100%|██████████| 1/1 [00:00<00:00, 50.52it/s, loss=8.35e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=5.93e+3]\n",
      "Epoch 516:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.24e+3, train_loss_epoch=8.24e+3, val_loss=5.93e+3]        \n",
      "Epoch 518:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.12e+3, train_loss_epoch=8.12e+3, val_loss=5.93e+3]        \n",
      "Epoch 518: 100%|██████████| 1/1 [00:00<00:00, 46.92it/s, loss=8.31e+03, v_num=0, train_loss_step=8.12e+3, train_loss_epoch=8.12e+3, val_loss=5.93e+3]\n",
      "Epoch 520: 100%|██████████| 1/1 [00:00<00:00, 50.10it/s, loss=8.32e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=5.93e+3]\n",
      "Epoch 523:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.22e+3, train_loss_epoch=8.22e+3, val_loss=5.93e+3]        \n",
      "Epoch 525:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=5.93e+3]        \n",
      "Epoch 527: 100%|██████████| 1/1 [00:00<00:00, 48.69it/s, loss=8.3e+03, v_num=0, train_loss_step=7.83e+3, train_loss_epoch=7.83e+3, val_loss=5.93e+3] \n",
      "Epoch 529: 100%|██████████| 1/1 [00:00<00:00, 49.62it/s, loss=8.28e+03, v_num=0, train_loss_step=7.63e+3, train_loss_epoch=7.63e+3, val_loss=5.93e+3]\n",
      "Epoch 532:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.54e+3, train_loss_epoch=8.54e+3, val_loss=5.93e+3]        \n",
      "Epoch 534: 100%|██████████| 1/1 [00:00<00:00, 48.54it/s, loss=8.25e+03, v_num=0, train_loss_step=8.43e+3, train_loss_epoch=8.43e+3, val_loss=5.93e+3]\n",
      "Epoch 536: 100%|██████████| 1/1 [00:00<00:00, 48.25it/s, loss=8.33e+03, v_num=0, train_loss_step=8.4e+3, train_loss_epoch=8.4e+3, val_loss=5.93e+3]  \n",
      "Epoch 539:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.38e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.31e+3, val_loss=5.93e+3]        \n",
      "Epoch 541:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.37e+03, v_num=0, train_loss_step=8.07e+3, train_loss_epoch=8.07e+3, val_loss=5.93e+3]        \n",
      "Epoch 541: 100%|██████████| 1/1 [00:00<00:00, 49.06it/s, loss=8.37e+03, v_num=0, train_loss_step=8.07e+3, train_loss_epoch=8.07e+3, val_loss=5.93e+3]\n",
      "Epoch 543: 100%|██████████| 1/1 [00:00<00:00, 50.93it/s, loss=8.36e+03, v_num=0, train_loss_step=7.88e+3, train_loss_epoch=7.88e+3, val_loss=5.93e+3]\n",
      "Epoch 546:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.31e+3, val_loss=5.93e+3]        \n",
      "Epoch 548:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.35e+03, v_num=0, train_loss_step=8.02e+3, train_loss_epoch=8.02e+3, val_loss=5.93e+3]        \n",
      "Epoch 550: 100%|██████████| 1/1 [00:00<00:00, 50.11it/s, loss=8.36e+03, v_num=0, train_loss_step=7.9e+3, train_loss_epoch=7.9e+3, val_loss=5.93e+3]  \n",
      "Epoch 552: 100%|██████████| 1/1 [00:00<00:00, 50.38it/s, loss=8.33e+03, v_num=0, train_loss_step=8.5e+3, train_loss_epoch=8.5e+3, val_loss=5.93e+3]  \n",
      "Epoch 552: 100%|██████████| 1/1 [00:00<00:00, 24.08it/s, loss=8.34e+03, v_num=0, train_loss_step=8.28e+3, train_loss_epoch=8.5e+3, val_loss=5.93e+3]\n",
      "Epoch 552: 100%|██████████| 1/1 [00:00<00:00, 23.68it/s, loss=8.34e+03, v_num=0, train_loss_step=8.28e+3, train_loss_epoch=8.28e+3, val_loss=5.93e+3]\n",
      "Epoch 553:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.28e+3, train_loss_epoch=8.28e+3, val_loss=5.93e+3]        \n",
      "Epoch 555:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.93e+3]        \n",
      "Epoch 557: 100%|██████████| 1/1 [00:00<00:00, 50.25it/s, loss=8.26e+03, v_num=0, train_loss_step=8.09e+3, train_loss_epoch=8.09e+3, val_loss=5.93e+3]\n",
      "Epoch 559: 100%|██████████| 1/1 [00:00<00:00, 23.44it/s, loss=8.25e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=5.93e+3]\n",
      "Epoch 560:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=5.93e+3]        \n",
      "Epoch 562:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.46e+3, train_loss_epoch=8.46e+3, val_loss=5.93e+3]        \n",
      "Epoch 564: 100%|██████████| 1/1 [00:00<00:00, 49.99it/s, loss=8.34e+03, v_num=0, train_loss_step=8.55e+3, train_loss_epoch=8.55e+3, val_loss=5.93e+3]\n",
      "Epoch 567:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.22e+3, train_loss_epoch=8.22e+3, val_loss=5.93e+3]        \n",
      "Epoch 569:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.37e+03, v_num=0, train_loss_step=8.83e+3, train_loss_epoch=8.83e+3, val_loss=5.93e+3]        \n",
      "Epoch 571: 100%|██████████| 1/1 [00:00<00:00, 51.42it/s, loss=8.38e+03, v_num=0, train_loss_step=8.19e+3, train_loss_epoch=8.19e+3, val_loss=5.93e+3]\n",
      "Epoch 574:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.41e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=5.93e+3]        \n",
      "Epoch 576:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=5.93e+3]        \n",
      "Epoch 576: 100%|██████████| 1/1 [00:00<00:00, 49.28it/s, loss=8.39e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=5.93e+3]\n",
      "Epoch 578: 100%|██████████| 1/1 [00:00<00:00, 38.59it/s, loss=8.35e+03, v_num=0, train_loss_step=8.08e+3, train_loss_epoch=8.08e+3, val_loss=5.93e+3]\n",
      "Epoch 581:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.93e+3]        \n",
      "Epoch 583: 100%|██████████| 1/1 [00:00<00:00, 50.33it/s, loss=8.28e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=5.93e+3]\n",
      "Epoch 585: 100%|██████████| 1/1 [00:00<00:00, 51.86it/s, loss=8.24e+03, v_num=0, train_loss_step=8.07e+3, train_loss_epoch=8.07e+3, val_loss=5.93e+3]\n",
      "Epoch 588:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.27e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=5.93e+3]        \n",
      "Epoch 590: 100%|██████████| 1/1 [00:00<00:00, 51.40it/s, loss=8.27e+03, v_num=0, train_loss_step=8.69e+3, train_loss_epoch=8.69e+3, val_loss=5.93e+3]\n",
      "Epoch 592: 100%|██████████| 1/1 [00:00<00:00, 48.18it/s, loss=8.25e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.93e+3]\n",
      "Epoch 595:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.24e+03, v_num=0, train_loss_step=8.5e+3, train_loss_epoch=8.5e+3, val_loss=5.93e+3]          \n",
      "Epoch 597: 100%|██████████| 1/1 [00:00<00:00, 48.91it/s, loss=8.28e+03, v_num=0, train_loss_step=8.58e+3, train_loss_epoch=8.58e+3, val_loss=5.93e+3]\n",
      "Epoch 599:  50%|█████     | 1/2 [00:00<00:00, 48.71it/s, loss=8.28e+03, v_num=0, train_loss_step=8.15e+3, train_loss_epoch=8.15e+3, val_loss=5.93e+3]\n",
      "Epoch 599:  50%|█████     | 1/2 [00:00<00:00, 22.40it/s, loss=8.29e+03, v_num=0, train_loss_step=7.81e+3, train_loss_epoch=8.15e+3, val_loss=5.93e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 599: 100%|██████████| 2/2 [00:00<00:00, 37.08it/s, loss=8.29e+03, v_num=0, train_loss_step=7.81e+3, train_loss_epoch=8.15e+3, val_loss=5.78e+3]\n",
      "Epoch 601: 100%|██████████| 1/1 [00:00<00:00, 50.15it/s, loss=8.3e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=5.78e+3] \n",
      "Epoch 604:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.23e+3, train_loss_epoch=8.23e+3, val_loss=5.78e+3]        \n",
      "Epoch 606:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.33e+03, v_num=0, train_loss_step=8.91e+3, train_loss_epoch=8.91e+3, val_loss=5.78e+3]        \n",
      "Epoch 608: 100%|██████████| 1/1 [00:00<00:00, 48.81it/s, loss=8.31e+03, v_num=0, train_loss_step=8.59e+3, train_loss_epoch=8.59e+3, val_loss=5.78e+3]\n",
      "Epoch 610: 100%|██████████| 1/1 [00:00<00:00, 50.10it/s, loss=8.28e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=5.78e+3]\n",
      "Epoch 613:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.72e+3, train_loss_epoch=8.72e+3, val_loss=5.78e+3]        \n",
      "Epoch 615: 100%|██████████| 1/1 [00:00<00:00, 52.04it/s, loss=8.33e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=5.78e+3]\n",
      "Epoch 617: 100%|██████████| 1/1 [00:00<00:00, 49.07it/s, loss=8.34e+03, v_num=0, train_loss_step=8.25e+3, train_loss_epoch=8.25e+3, val_loss=5.78e+3]\n",
      "Epoch 620:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.66e+3, train_loss_epoch=8.66e+3, val_loss=5.78e+3]        \n",
      "Epoch 622: 100%|██████████| 1/1 [00:00<00:00, 50.79it/s, loss=8.38e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=5.78e+3]\n",
      "Epoch 624: 100%|██████████| 1/1 [00:00<00:00, 23.75it/s, loss=8.4e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=5.78e+3] \n",
      "Epoch 625:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.4e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=5.78e+3]        \n",
      "Epoch 627:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.37e+03, v_num=0, train_loss_step=7.99e+3, train_loss_epoch=7.99e+3, val_loss=5.78e+3]        \n",
      "Epoch 629: 100%|██████████| 1/1 [00:00<00:00, 48.76it/s, loss=8.35e+03, v_num=0, train_loss_step=8.49e+3, train_loss_epoch=8.49e+3, val_loss=5.78e+3]\n",
      "Epoch 632:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.38e+03, v_num=0, train_loss_step=7.88e+3, train_loss_epoch=7.88e+3, val_loss=5.78e+3]        \n",
      "Epoch 634:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=7.68e+3, train_loss_epoch=7.68e+3, val_loss=5.78e+3]        \n",
      "Epoch 636: 100%|██████████| 1/1 [00:00<00:00, 49.13it/s, loss=8.28e+03, v_num=0, train_loss_step=8.4e+3, train_loss_epoch=8.4e+3, val_loss=5.78e+3]  \n",
      "Epoch 638: 100%|██████████| 1/1 [00:00<00:00, 50.70it/s, loss=8.23e+03, v_num=0, train_loss_step=7.93e+3, train_loss_epoch=7.93e+3, val_loss=5.78e+3]\n",
      "Epoch 641:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.22e+03, v_num=0, train_loss_step=8.66e+3, train_loss_epoch=8.66e+3, val_loss=5.78e+3]        \n",
      "Epoch 643: 100%|██████████| 1/1 [00:00<00:00, 50.46it/s, loss=8.2e+03, v_num=0, train_loss_step=7.95e+3, train_loss_epoch=7.95e+3, val_loss=5.78e+3] \n",
      "Epoch 645: 100%|██████████| 1/1 [00:00<00:00, 50.89it/s, loss=8.23e+03, v_num=0, train_loss_step=8.77e+3, train_loss_epoch=8.77e+3, val_loss=5.78e+3]\n",
      "Epoch 648:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=8.49e+3, train_loss_epoch=8.49e+3, val_loss=5.78e+3]        \n",
      "Epoch 650: 100%|██████████| 1/1 [00:00<00:00, 50.95it/s, loss=8.26e+03, v_num=0, train_loss_step=8818.25, train_loss_epoch=8818.25, val_loss=5.78e+3]\n",
      "Epoch 652: 100%|██████████| 1/1 [00:00<00:00, 52.17it/s, loss=8.25e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.78e+3]\n",
      "Epoch 655:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.04e+3, train_loss_epoch=8.04e+3, val_loss=5.78e+3]        \n",
      "Epoch 657: 100%|██████████| 1/1 [00:00<00:00, 51.13it/s, loss=8.37e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.31e+3, val_loss=5.78e+3]\n",
      "Epoch 659: 100%|██████████| 1/1 [00:00<00:00, 23.89it/s, loss=8.39e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.16e+3, val_loss=5.78e+3]\n",
      "Epoch 659: 100%|██████████| 1/1 [00:00<00:00, 23.55it/s, loss=8.39e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.31e+3, val_loss=5.78e+3]\n",
      "Epoch 660:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.31e+3, val_loss=5.78e+3]        \n",
      "Epoch 662:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.42e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.78e+3]        \n",
      "Epoch 664: 100%|██████████| 1/1 [00:00<00:00, 51.46it/s, loss=8.44e+03, v_num=0, train_loss_step=8.77e+3, train_loss_epoch=8.77e+3, val_loss=5.78e+3]\n",
      "Epoch 666: 100%|██████████| 1/1 [00:00<00:00, 22.28it/s, loss=8.41e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.43e+3, val_loss=5.78e+3]\n",
      "Epoch 666: 100%|██████████| 1/1 [00:00<00:00, 21.97it/s, loss=8.41e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.78e+3]\n",
      "Epoch 667:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.41e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.78e+3]        \n",
      "Epoch 669:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.48e+03, v_num=0, train_loss_step=8.75e+3, train_loss_epoch=8.75e+3, val_loss=5.78e+3]        \n",
      "Epoch 671: 100%|██████████| 1/1 [00:00<00:00, 49.17it/s, loss=8.45e+03, v_num=0, train_loss_step=8.28e+3, train_loss_epoch=8.28e+3, val_loss=5.78e+3]\n",
      "Epoch 673: 100%|██████████| 1/1 [00:00<00:00, 50.56it/s, loss=8.39e+03, v_num=0, train_loss_step=8.11e+3, train_loss_epoch=8.11e+3, val_loss=5.78e+3]\n",
      "Epoch 676:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=5.78e+3]        \n",
      "Epoch 678:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.78e+3]        \n",
      "Epoch 680: 100%|██████████| 1/1 [00:00<00:00, 48.96it/s, loss=8.31e+03, v_num=0, train_loss_step=7.71e+3, train_loss_epoch=7.71e+3, val_loss=5.78e+3]\n",
      "Epoch 682: 100%|██████████| 1/1 [00:00<00:00, 49.10it/s, loss=8.29e+03, v_num=0, train_loss_step=8.22e+3, train_loss_epoch=8.22e+3, val_loss=5.78e+3]\n",
      "Epoch 685:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.14e+3, train_loss_epoch=8.14e+3, val_loss=5.78e+3]        \n",
      "Epoch 687: 100%|██████████| 1/1 [00:00<00:00, 52.11it/s, loss=8.28e+03, v_num=0, train_loss_step=8.23e+3, train_loss_epoch=8.23e+3, val_loss=5.78e+3]\n",
      "Epoch 689: 100%|██████████| 1/1 [00:00<00:00, 47.76it/s, loss=8.27e+03, v_num=0, train_loss_step=8.57e+3, train_loss_epoch=8.57e+3, val_loss=5.78e+3]\n",
      "Epoch 692:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=8.24e+3, train_loss_epoch=8.24e+3, val_loss=5.78e+3]        \n",
      "Epoch 694:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.23e+03, v_num=0, train_loss_step=7.72e+3, train_loss_epoch=7.72e+3, val_loss=5.78e+3]        \n",
      "Epoch 696: 100%|██████████| 1/1 [00:00<00:00, 49.29it/s, loss=8.22e+03, v_num=0, train_loss_step=7.99e+3, train_loss_epoch=7.99e+3, val_loss=5.78e+3]\n",
      "Epoch 698: 100%|██████████| 1/1 [00:00<00:00, 50.59it/s, loss=8.19e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=5.78e+3]\n",
      "Epoch 699:  50%|█████     | 1/2 [00:00<00:00, 24.18it/s, loss=8.22e+03, v_num=0, train_loss_step=8.16e+3, train_loss_epoch=8.12e+3, val_loss=5.78e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 699: 100%|██████████| 2/2 [00:00<00:00, 40.10it/s, loss=8.22e+03, v_num=0, train_loss_step=8.16e+3, train_loss_epoch=8.12e+3, val_loss=5.69e+3]\n",
      "Epoch 700: 100%|██████████| 1/1 [00:00<00:00, 23.55it/s, loss=8.18e+03, v_num=0, train_loss_step=7.71e+3, train_loss_epoch=7.71e+3, val_loss=5.69e+3]\n",
      "Epoch 701:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.18e+03, v_num=0, train_loss_step=7.71e+3, train_loss_epoch=7.71e+3, val_loss=5.69e+3]        \n",
      "Epoch 703:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.16e+03, v_num=0, train_loss_step=7.8e+3, train_loss_epoch=7.8e+3, val_loss=5.69e+3]          \n",
      "Epoch 705: 100%|██████████| 1/1 [00:00<00:00, 47.19it/s, loss=8.17e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=5.69e+3]\n",
      "Epoch 707: 100%|██████████| 1/1 [00:00<00:00, 51.00it/s, loss=8.14e+03, v_num=0, train_loss_step=8.09e+3, train_loss_epoch=8.09e+3, val_loss=5.69e+3]\n",
      "Epoch 707: 100%|██████████| 1/1 [00:00<00:00, 23.79it/s, loss=8.13e+03, v_num=0, train_loss_step=8.5e+3, train_loss_epoch=8.09e+3, val_loss=5.69e+3] \n",
      "Epoch 708:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.13e+03, v_num=0, train_loss_step=8.5e+3, train_loss_epoch=8.5e+3, val_loss=5.69e+3]         \n",
      "Epoch 710:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.14e+03, v_num=0, train_loss_step=8.73e+3, train_loss_epoch=8.73e+3, val_loss=5.69e+3]        \n",
      "Epoch 712: 100%|██████████| 1/1 [00:00<00:00, 50.81it/s, loss=8.19e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=5.69e+3]\n",
      "Epoch 715:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.27e+03, v_num=0, train_loss_step=8.45e+3, train_loss_epoch=8.45e+3, val_loss=5.69e+3]        \n",
      "Epoch 717:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.3e+03, v_num=0, train_loss_step=7.85e+3, train_loss_epoch=7.85e+3, val_loss=5.69e+3]         \n",
      "Epoch 719: 100%|██████████| 1/1 [00:00<00:00, 51.19it/s, loss=8.32e+03, v_num=0, train_loss_step=8.54e+3, train_loss_epoch=8.54e+3, val_loss=5.69e+3]\n",
      "Epoch 722:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.38e+03, v_num=0, train_loss_step=8.94e+3, train_loss_epoch=8.94e+3, val_loss=5.69e+3]        \n",
      "Epoch 724:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.42e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.69e+3]        \n",
      "Epoch 724: 100%|██████████| 1/1 [00:00<00:00, 51.43it/s, loss=8.42e+03, v_num=0, train_loss_step=8.37e+3, train_loss_epoch=8.37e+3, val_loss=5.69e+3]\n",
      "Epoch 726: 100%|██████████| 1/1 [00:00<00:00, 50.92it/s, loss=8.4e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.31e+3, val_loss=5.69e+3] \n",
      "Epoch 729:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.18e+3, train_loss_epoch=8.18e+3, val_loss=5.69e+3]        \n",
      "Epoch 731:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.33e+03, v_num=0, train_loss_step=7.86e+3, train_loss_epoch=7.86e+3, val_loss=5.69e+3]        \n",
      "Epoch 731: 100%|██████████| 1/1 [00:00<00:00, 48.55it/s, loss=8.33e+03, v_num=0, train_loss_step=7.86e+3, train_loss_epoch=7.86e+3, val_loss=5.69e+3]\n",
      "Epoch 733: 100%|██████████| 1/1 [00:00<00:00, 52.30it/s, loss=8.33e+03, v_num=0, train_loss_step=8.57e+3, train_loss_epoch=8.57e+3, val_loss=5.69e+3]\n",
      "Epoch 736:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=5.69e+3]        \n",
      "Epoch 738: 100%|██████████| 1/1 [00:00<00:00, 48.69it/s, loss=8.36e+03, v_num=0, train_loss_step=8.96e+3, train_loss_epoch=8.96e+3, val_loss=5.69e+3]\n",
      "Epoch 740: 100%|██████████| 1/1 [00:00<00:00, 49.12it/s, loss=8.39e+03, v_num=0, train_loss_step=8.45e+3, train_loss_epoch=8.45e+3, val_loss=5.69e+3]\n",
      "Epoch 742: 100%|██████████| 1/1 [00:00<00:00, 47.54it/s, loss=8.33e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.69e+3]\n",
      "Epoch 745:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.3e+3, train_loss_epoch=8.3e+3, val_loss=5.69e+3]          \n",
      "Epoch 747: 100%|██████████| 1/1 [00:00<00:00, 48.80it/s, loss=8.32e+03, v_num=0, train_loss_step=8.61e+3, train_loss_epoch=8.61e+3, val_loss=5.69e+3]\n",
      "Epoch 749: 100%|██████████| 1/1 [00:00<00:00, 49.85it/s, loss=8.31e+03, v_num=0, train_loss_step=8.32e+3, train_loss_epoch=8.32e+3, val_loss=5.69e+3]\n",
      "Epoch 752:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.35e+03, v_num=0, train_loss_step=8.41e+3, train_loss_epoch=8.41e+3, val_loss=5.69e+3]        \n",
      "Epoch 754:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.23e+3, train_loss_epoch=8.23e+3, val_loss=5.69e+3]        \n",
      "Epoch 756: 100%|██████████| 1/1 [00:00<00:00, 50.97it/s, loss=8.33e+03, v_num=0, train_loss_step=8.19e+3, train_loss_epoch=8.19e+3, val_loss=5.69e+3]\n",
      "Epoch 758: 100%|██████████| 1/1 [00:00<00:00, 51.21it/s, loss=8.31e+03, v_num=0, train_loss_step=8.63e+3, train_loss_epoch=8.63e+3, val_loss=5.69e+3]\n",
      "Epoch 758: 100%|██████████| 1/1 [00:00<00:00, 23.64it/s, loss=8.26e+03, v_num=0, train_loss_step=7.6e+3, train_loss_epoch=7.6e+3, val_loss=5.69e+3]  \n",
      "Epoch 759:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=7.6e+3, train_loss_epoch=7.6e+3, val_loss=5.69e+3]        \n",
      "Epoch 761:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=5.69e+3]        \n",
      "Epoch 763: 100%|██████████| 1/1 [00:00<00:00, 49.32it/s, loss=8.28e+03, v_num=0, train_loss_step=8.14e+3, train_loss_epoch=8.14e+3, val_loss=5.69e+3]\n",
      "Epoch 765: 100%|██████████| 1/1 [00:00<00:00, 48.85it/s, loss=8.27e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=5.69e+3]\n",
      "Epoch 768:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.16e+3, train_loss_epoch=8.16e+3, val_loss=5.69e+3]        \n",
      "Epoch 770: 100%|██████████| 1/1 [00:00<00:00, 51.52it/s, loss=8.29e+03, v_num=0, train_loss_step=8.79e+3, train_loss_epoch=8.79e+3, val_loss=5.69e+3]\n",
      "Epoch 772: 100%|██████████| 1/1 [00:00<00:00, 51.35it/s, loss=8.26e+03, v_num=0, train_loss_step=8.5e+3, train_loss_epoch=8.5e+3, val_loss=5.69e+3]  \n",
      "Epoch 775:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=8.25e+3, train_loss_epoch=8.25e+3, val_loss=5.69e+3]        \n",
      "Epoch 777: 100%|██████████| 1/1 [00:00<00:00, 49.72it/s, loss=8.31e+03, v_num=0, train_loss_step=8.87e+3, train_loss_epoch=8.87e+3, val_loss=5.69e+3]\n",
      "Epoch 779: 100%|██████████| 1/1 [00:00<00:00, 50.48it/s, loss=8.31e+03, v_num=0, train_loss_step=8.24e+3, train_loss_epoch=8.24e+3, val_loss=5.69e+3]\n",
      "Epoch 782:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.3e+03, v_num=0, train_loss_step=8.23e+3, train_loss_epoch=8.23e+3, val_loss=5.69e+3]         \n",
      "Epoch 784: 100%|██████████| 1/1 [00:00<00:00, 50.77it/s, loss=8.37e+03, v_num=0, train_loss_step=8.83e+3, train_loss_epoch=8.83e+3, val_loss=5.69e+3]\n",
      "Epoch 786: 100%|██████████| 1/1 [00:00<00:00, 50.44it/s, loss=8.36e+03, v_num=0, train_loss_step=8.51e+3, train_loss_epoch=8.51e+3, val_loss=5.69e+3]\n",
      "Epoch 789:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.4e+03, v_num=0, train_loss_step=8.2e+3, train_loss_epoch=8.2e+3, val_loss=5.69e+3]           \n",
      "Epoch 791: 100%|██████████| 1/1 [00:00<00:00, 48.74it/s, loss=8.4e+03, v_num=0, train_loss_step=8.35e+3, train_loss_epoch=8.35e+3, val_loss=5.69e+3] \n",
      "Epoch 793: 100%|██████████| 1/1 [00:00<00:00, 52.05it/s, loss=8.39e+03, v_num=0, train_loss_step=8.14e+3, train_loss_epoch=8.14e+3, val_loss=5.69e+3]\n",
      "Epoch 796:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.37e+03, v_num=0, train_loss_step=7.98e+3, train_loss_epoch=7.98e+3, val_loss=5.69e+3]        \n",
      "Epoch 798: 100%|██████████| 1/1 [00:00<00:00, 50.46it/s, loss=8.3e+03, v_num=0, train_loss_step=7.91e+3, train_loss_epoch=7.91e+3, val_loss=5.69e+3] \n",
      "Epoch 799:  50%|█████     | 1/2 [00:00<00:00, 22.77it/s, loss=8.33e+03, v_num=0, train_loss_step=8.97e+3, train_loss_epoch=8.17e+3, val_loss=5.69e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 799: 100%|██████████| 2/2 [00:00<00:00, 38.17it/s, loss=8.33e+03, v_num=0, train_loss_step=8.97e+3, train_loss_epoch=8.17e+3, val_loss=5.76e+3]\n",
      "Epoch 800:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.33e+03, v_num=0, train_loss_step=8.97e+3, train_loss_epoch=8.97e+3, val_loss=5.76e+3]        \n",
      "Epoch 802: 100%|██████████| 1/1 [00:00<00:00, 47.81it/s, loss=8.33e+03, v_num=0, train_loss_step=8.38e+3, train_loss_epoch=8.38e+3, val_loss=5.76e+3]\n",
      "Epoch 805:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.48e+3, train_loss_epoch=8.48e+3, val_loss=5.76e+3]        \n",
      "Epoch 807:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=7.87e+3, train_loss_epoch=7.87e+3, val_loss=5.76e+3]        \n",
      "Epoch 809: 100%|██████████| 1/1 [00:00<00:00, 48.39it/s, loss=8.19e+03, v_num=0, train_loss_step=8.04e+3, train_loss_epoch=8.04e+3, val_loss=5.76e+3]\n",
      "Epoch 811: 100%|██████████| 1/1 [00:00<00:00, 50.89it/s, loss=8.18e+03, v_num=0, train_loss_step=8.38e+3, train_loss_epoch=8.38e+3, val_loss=5.76e+3]\n",
      "Epoch 814:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.19e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=5.76e+3]        \n",
      "Epoch 816: 100%|██████████| 1/1 [00:00<00:00, 51.36it/s, loss=8.21e+03, v_num=0, train_loss_step=8.09e+3, train_loss_epoch=8.09e+3, val_loss=5.76e+3]\n",
      "Epoch 818: 100%|██████████| 1/1 [00:00<00:00, 49.49it/s, loss=8.29e+03, v_num=0, train_loss_step=9.25e+3, train_loss_epoch=9.25e+3, val_loss=5.76e+3]\n",
      "Epoch 821:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=8.31e+3, train_loss_epoch=8.31e+3, val_loss=5.76e+3]        \n",
      "Epoch 823:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=7.96e+3, train_loss_epoch=7.96e+3, val_loss=5.76e+3]        \n",
      "Epoch 825:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.21e+03, v_num=0, train_loss_step=8.46e+3, train_loss_epoch=8.46e+3, val_loss=5.76e+3]        \n",
      "Epoch 825: 100%|██████████| 1/1 [00:00<00:00, 48.20it/s, loss=8.21e+03, v_num=0, train_loss_step=8.46e+3, train_loss_epoch=8.46e+3, val_loss=5.76e+3]\n",
      "Epoch 827: 100%|██████████| 1/1 [00:00<00:00, 50.52it/s, loss=8.27e+03, v_num=0, train_loss_step=8.58e+3, train_loss_epoch=8.58e+3, val_loss=5.76e+3]\n",
      "Epoch 830:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.19e+3, train_loss_epoch=8.19e+3, val_loss=5.76e+3]        \n",
      "Epoch 832: 100%|██████████| 1/1 [00:00<00:00, 49.25it/s, loss=8.32e+03, v_num=0, train_loss_step=8.57e+3, train_loss_epoch=8.57e+3, val_loss=5.76e+3]\n",
      "Epoch 834: 100%|██████████| 1/1 [00:00<00:00, 49.38it/s, loss=8.32e+03, v_num=0, train_loss_step=7.97e+3, train_loss_epoch=7.97e+3, val_loss=5.76e+3]\n",
      "Epoch 836: 100%|██████████| 1/1 [00:00<00:00, 49.27it/s, loss=8.3e+03, v_num=0, train_loss_step=8.07e+3, train_loss_epoch=8.07e+3, val_loss=5.76e+3] \n",
      "Epoch 839:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=7.92e+3, train_loss_epoch=7.92e+3, val_loss=5.76e+3]        \n",
      "Epoch 841:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.92e+3, train_loss_epoch=8.92e+3, val_loss=5.76e+3]        \n",
      "Epoch 843: 100%|██████████| 1/1 [00:00<00:00, 51.38it/s, loss=8.31e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.76e+3]\n",
      "Epoch 846:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.38e+03, v_num=0, train_loss_step=8.67e+3, train_loss_epoch=8.67e+3, val_loss=5.76e+3]        \n",
      "Epoch 848:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=7.58e+3, train_loss_epoch=7.58e+3, val_loss=5.76e+3]        \n",
      "Epoch 848: 100%|██████████| 1/1 [00:00<00:00, 49.75it/s, loss=8.36e+03, v_num=0, train_loss_step=7.58e+3, train_loss_epoch=7.58e+3, val_loss=5.76e+3]\n",
      "Epoch 850: 100%|██████████| 1/1 [00:00<00:00, 50.92it/s, loss=8.3e+03, v_num=0, train_loss_step=7.92e+3, train_loss_epoch=7.92e+3, val_loss=5.76e+3] \n",
      "Epoch 853:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.24e+03, v_num=0, train_loss_step=7.95e+3, train_loss_epoch=7.95e+3, val_loss=5.76e+3]        \n",
      "Epoch 855: 100%|██████████| 1/1 [00:00<00:00, 51.38it/s, loss=8.23e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=5.76e+3]\n",
      "Epoch 857: 100%|██████████| 1/1 [00:00<00:00, 50.85it/s, loss=8.24e+03, v_num=0, train_loss_step=8.21e+3, train_loss_epoch=8.21e+3, val_loss=5.76e+3]\n",
      "Epoch 860:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.21e+03, v_num=0, train_loss_step=8.27e+3, train_loss_epoch=8.27e+3, val_loss=5.76e+3]        \n",
      "Epoch 862: 100%|██████████| 1/1 [00:00<00:00, 48.05it/s, loss=8.2e+03, v_num=0, train_loss_step=8.6e+3, train_loss_epoch=8.6e+3, val_loss=5.76e+3]   \n",
      "Epoch 864: 100%|██████████| 1/1 [00:00<00:00, 49.31it/s, loss=8.19e+03, v_num=0, train_loss_step=8.05e+3, train_loss_epoch=8.05e+3, val_loss=5.76e+3]\n",
      "Epoch 866: 100%|██████████| 1/1 [00:00<00:00, 49.29it/s, loss=8.13e+03, v_num=0, train_loss_step=8.12e+3, train_loss_epoch=8.12e+3, val_loss=5.76e+3]\n",
      "Epoch 869:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.14e+03, v_num=0, train_loss_step=8.36e+3, train_loss_epoch=8.36e+3, val_loss=5.76e+3]        \n",
      "Epoch 871: 100%|██████████| 1/1 [00:00<00:00, 50.54it/s, loss=8.18e+03, v_num=0, train_loss_step=8.83e+3, train_loss_epoch=8.83e+3, val_loss=5.76e+3]\n",
      "Epoch 873: 100%|██████████| 1/1 [00:00<00:00, 48.87it/s, loss=8.19e+03, v_num=0, train_loss_step=8.18e+3, train_loss_epoch=8.18e+3, val_loss=5.76e+3]\n",
      "Epoch 875: 100%|██████████| 1/1 [00:00<00:00, 21.82it/s, loss=8.21e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.29e+3, val_loss=5.76e+3]\n",
      "Epoch 875:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.21e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=5.76e+3]        \n",
      "Epoch 876:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.21e+03, v_num=0, train_loss_step=8.42e+3, train_loss_epoch=8.42e+3, val_loss=5.76e+3]\n",
      "Epoch 878:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.24e+03, v_num=0, train_loss_step=8.39e+3, train_loss_epoch=8.39e+3, val_loss=5.76e+3]        \n",
      "Epoch 880: 100%|██████████| 1/1 [00:00<00:00, 51.81it/s, loss=8.24e+03, v_num=0, train_loss_step=8.03e+3, train_loss_epoch=8.03e+3, val_loss=5.76e+3]\n",
      "Epoch 882: 100%|██████████| 1/1 [00:00<00:00, 51.04it/s, loss=8.23e+03, v_num=0, train_loss_step=8.2e+3, train_loss_epoch=8.2e+3, val_loss=5.76e+3]  \n",
      "Epoch 885:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.27e+03, v_num=0, train_loss_step=8.59e+3, train_loss_epoch=8.59e+3, val_loss=5.76e+3]        \n",
      "Epoch 887: 100%|██████████| 1/1 [00:00<00:00, 49.17it/s, loss=8.24e+03, v_num=0, train_loss_step=8.01e+3, train_loss_epoch=8.01e+3, val_loss=5.76e+3]\n",
      "Epoch 889: 100%|██████████| 1/1 [00:00<00:00, 48.75it/s, loss=8.25e+03, v_num=0, train_loss_step=7.98e+3, train_loss_epoch=7.98e+3, val_loss=5.76e+3]\n",
      "Epoch 891: 100%|██████████| 1/1 [00:00<00:00, 50.07it/s, loss=8.27e+03, v_num=0, train_loss_step=8.43e+3, train_loss_epoch=8.43e+3, val_loss=5.76e+3]\n",
      "Epoch 894:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.33e+03, v_num=0, train_loss_step=8.16e+3, train_loss_epoch=8.16e+3, val_loss=5.76e+3]        \n",
      "Epoch 896: 100%|██████████| 1/1 [00:00<00:00, 51.36it/s, loss=8.3e+03, v_num=0, train_loss_step=7.94e+3, train_loss_epoch=7.94e+3, val_loss=5.76e+3] \n",
      "Epoch 898: 100%|██████████| 1/1 [00:00<00:00, 51.77it/s, loss=8.3e+03, v_num=0, train_loss_step=8.39e+3, train_loss_epoch=8.39e+3, val_loss=5.76e+3]\n",
      "Epoch 898: 100%|██████████| 1/1 [00:00<00:00, 23.37it/s, loss=8.29e+03, v_num=0, train_loss_step=8.1e+3, train_loss_epoch=8.1e+3, val_loss=5.76e+3] \n",
      "Epoch 899:   0%|          | 0/2 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=8.1e+3, train_loss_epoch=8.1e+3, val_loss=5.76e+3]        \n",
      "Epoch 899:  50%|█████     | 1/2 [00:00<00:00, 22.88it/s, loss=8.32e+03, v_num=0, train_loss_step=8.58e+3, train_loss_epoch=8.1e+3, val_loss=5.76e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 899: 100%|██████████| 2/2 [00:00<00:00, 38.13it/s, loss=8.32e+03, v_num=0, train_loss_step=8.58e+3, train_loss_epoch=8.1e+3, val_loss=5.74e+3]\n",
      "Epoch 900: 100%|██████████| 1/1 [00:00<00:00, 22.23it/s, loss=8.29e+03, v_num=0, train_loss_step=7.82e+3, train_loss_epoch=8.58e+3, val_loss=5.74e+3]\n",
      "Epoch 900: 100%|██████████| 1/1 [00:00<00:00, 21.96it/s, loss=8.29e+03, v_num=0, train_loss_step=7.82e+3, train_loss_epoch=7.82e+3, val_loss=5.74e+3]\n",
      "Epoch 900:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=7.82e+3, train_loss_epoch=7.82e+3, val_loss=5.74e+3]        \n",
      "Epoch 901:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=7.82e+3, train_loss_epoch=7.82e+3, val_loss=5.74e+3]\n",
      "Epoch 903:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.29e+03, v_num=0, train_loss_step=7.87e+3, train_loss_epoch=7.87e+3, val_loss=5.74e+3]        \n",
      "Epoch 905:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=8.04e+3, train_loss_epoch=8.04e+3, val_loss=5.74e+3]        \n",
      "Epoch 905: 100%|██████████| 1/1 [00:00<00:00, 50.02it/s, loss=8.25e+03, v_num=0, train_loss_step=8.04e+3, train_loss_epoch=8.04e+3, val_loss=5.74e+3]\n",
      "Epoch 907: 100%|██████████| 1/1 [00:00<00:00, 51.18it/s, loss=8.32e+03, v_num=0, train_loss_step=8.57e+3, train_loss_epoch=8.57e+3, val_loss=5.74e+3]\n",
      "Epoch 910:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.17e+3, train_loss_epoch=8.17e+3, val_loss=5.74e+3]        \n",
      "Epoch 912: 100%|██████████| 1/1 [00:00<00:00, 52.05it/s, loss=8.29e+03, v_num=0, train_loss_step=8.57e+3, train_loss_epoch=8.57e+3, val_loss=5.74e+3]\n",
      "Epoch 914: 100%|██████████| 1/1 [00:00<00:00, 48.29it/s, loss=8.29e+03, v_num=0, train_loss_step=7.97e+3, train_loss_epoch=7.97e+3, val_loss=5.74e+3]\n",
      "Epoch 916: 100%|██████████| 1/1 [00:00<00:00, 48.94it/s, loss=8.31e+03, v_num=0, train_loss_step=8.44e+3, train_loss_epoch=8.44e+3, val_loss=5.74e+3]\n",
      "Epoch 916: 100%|██████████| 1/1 [00:00<00:00, 22.52it/s, loss=8.31e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.44e+3, val_loss=5.74e+3]\n",
      "Epoch 916: 100%|██████████| 1/1 [00:00<00:00, 22.21it/s, loss=8.31e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=5.74e+3]\n",
      "Epoch 916:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=5.74e+3]        \n",
      "Epoch 917:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=5.74e+3]\n",
      "Epoch 919:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.28e+3, train_loss_epoch=8.28e+3, val_loss=5.74e+3]        \n",
      "Epoch 921:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.37e+03, v_num=0, train_loss_step=8.98e+3, train_loss_epoch=8.98e+3, val_loss=5.74e+3]        \n",
      "Epoch 923: 100%|██████████| 1/1 [00:00<00:00, 51.13it/s, loss=8.33e+03, v_num=0, train_loss_step=7.84e+3, train_loss_epoch=7.84e+3, val_loss=5.74e+3]\n",
      "Epoch 926:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.62e+3, train_loss_epoch=8.62e+3, val_loss=5.74e+3]        \n",
      "Epoch 928:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.41e+03, v_num=0, train_loss_step=8.88e+3, train_loss_epoch=8.88e+3, val_loss=5.74e+3]        \n",
      "Epoch 930: 100%|██████████| 1/1 [00:00<00:00, 48.36it/s, loss=8.44e+03, v_num=0, train_loss_step=7.93e+3, train_loss_epoch=7.93e+3, val_loss=5.74e+3]\n",
      "Epoch 932: 100%|██████████| 1/1 [00:00<00:00, 46.95it/s, loss=8.4e+03, v_num=0, train_loss_step=7.99e+3, train_loss_epoch=7.99e+3, val_loss=5.74e+3] \n",
      "Epoch 935:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.39e+03, v_num=0, train_loss_step=8.29e+3, train_loss_epoch=8.29e+3, val_loss=5.74e+3]        \n",
      "Epoch 937: 100%|██████████| 1/1 [00:00<00:00, 51.15it/s, loss=8.39e+03, v_num=0, train_loss_step=7.97e+3, train_loss_epoch=7.97e+3, val_loss=5.74e+3]\n",
      "Epoch 939: 100%|██████████| 1/1 [00:00<00:00, 51.41it/s, loss=8.37e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=5.74e+3]\n",
      "Epoch 942:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.28e+03, v_num=0, train_loss_step=8.14e+3, train_loss_epoch=8.14e+3, val_loss=5.74e+3]        \n",
      "Epoch 944:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.27e+03, v_num=0, train_loss_step=7.83e+3, train_loss_epoch=7.83e+3, val_loss=5.74e+3]        \n",
      "Epoch 946: 100%|██████████| 1/1 [00:00<00:00, 48.34it/s, loss=8.18e+03, v_num=0, train_loss_step=8.19e+3, train_loss_epoch=8.19e+3, val_loss=5.74e+3]\n",
      "Epoch 948: 100%|██████████| 1/1 [00:00<00:00, 51.36it/s, loss=8.16e+03, v_num=0, train_loss_step=8.11e+3, train_loss_epoch=8.11e+3, val_loss=5.74e+3]\n",
      "Epoch 951:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.18e+03, v_num=0, train_loss_step=8.52e+3, train_loss_epoch=8.52e+3, val_loss=5.74e+3]        \n",
      "Epoch 953:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=5.74e+3]        \n",
      "Epoch 953: 100%|██████████| 1/1 [00:00<00:00, 49.57it/s, loss=8.26e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=5.74e+3]\n",
      "Epoch 955: 100%|██████████| 1/1 [00:00<00:00, 48.80it/s, loss=8.25e+03, v_num=0, train_loss_step=8.17e+3, train_loss_epoch=8.17e+3, val_loss=5.74e+3]\n",
      "Epoch 957: 100%|██████████| 1/1 [00:00<00:00, 48.86it/s, loss=8.23e+03, v_num=0, train_loss_step=8.15e+3, train_loss_epoch=8.15e+3, val_loss=5.74e+3]\n",
      "Epoch 960:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.31e+03, v_num=0, train_loss_step=8.8e+3, train_loss_epoch=8.8e+3, val_loss=5.74e+3]          \n",
      "Epoch 962:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.34e+03, v_num=0, train_loss_step=8.7e+3, train_loss_epoch=8.7e+3, val_loss=5.74e+3]          \n",
      "Epoch 964: 100%|██████████| 1/1 [00:00<00:00, 48.55it/s, loss=8.32e+03, v_num=0, train_loss_step=7.77e+3, train_loss_epoch=7.77e+3, val_loss=5.74e+3]\n",
      "Epoch 966: 100%|██████████| 1/1 [00:00<00:00, 51.08it/s, loss=8.39e+03, v_num=0, train_loss_step=8.81e+3, train_loss_epoch=8.81e+3, val_loss=5.74e+3]\n",
      "Epoch 969:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.35e+03, v_num=0, train_loss_step=8.47e+3, train_loss_epoch=8.47e+3, val_loss=5.74e+3]        \n",
      "Epoch 971:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.32e+03, v_num=0, train_loss_step=8.01e+3, train_loss_epoch=8.01e+3, val_loss=5.74e+3]        \n",
      "Epoch 973: 100%|██████████| 1/1 [00:00<00:00, 50.61it/s, loss=8.27e+03, v_num=0, train_loss_step=8.15e+3, train_loss_epoch=8.15e+3, val_loss=5.74e+3]\n",
      "Epoch 976:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.25e+03, v_num=0, train_loss_step=8.24e+3, train_loss_epoch=8.24e+3, val_loss=5.74e+3]        \n",
      "Epoch 978:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.26e+03, v_num=0, train_loss_step=8.53e+3, train_loss_epoch=8.53e+3, val_loss=5.74e+3]        \n",
      "Epoch 980: 100%|██████████| 1/1 [00:00<00:00, 49.32it/s, loss=8.24e+03, v_num=0, train_loss_step=8.21e+3, train_loss_epoch=8.21e+3, val_loss=5.74e+3]\n",
      "Epoch 982: 100%|██████████| 1/1 [00:00<00:00, 48.54it/s, loss=8.28e+03, v_num=0, train_loss_step=8.66e+3, train_loss_epoch=8.66e+3, val_loss=5.74e+3]\n",
      "Epoch 985:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.35e+03, v_num=0, train_loss_step=8356.75, train_loss_epoch=8356.75, val_loss=5.74e+3]        \n",
      "Epoch 987:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.36e+03, v_num=0, train_loss_step=8.46e+3, train_loss_epoch=8.46e+3, val_loss=5.74e+3]        \n",
      "Epoch 989: 100%|██████████| 1/1 [00:00<00:00, 51.68it/s, loss=8.38e+03, v_num=0, train_loss_step=8.69e+3, train_loss_epoch=8.69e+3, val_loss=5.74e+3]\n",
      "Epoch 991: 100%|██████████| 1/1 [00:00<00:00, 51.29it/s, loss=8.38e+03, v_num=0, train_loss_step=8.39e+3, train_loss_epoch=8.39e+3, val_loss=5.74e+3]\n",
      "Epoch 994:   0%|          | 0/1 [00:00<?, ?it/s, loss=8.41e+03, v_num=0, train_loss_step=7.92e+3, train_loss_epoch=7.92e+3, val_loss=5.74e+3]        \n",
      "Epoch 996: 100%|██████████| 1/1 [00:00<00:00, 52.08it/s, loss=8.44e+03, v_num=0, train_loss_step=9.23e+3, train_loss_epoch=9.23e+3, val_loss=5.74e+3]\n",
      "Epoch 998: 100%|██████████| 1/1 [00:00<00:00, 50.71it/s, loss=8.51e+03, v_num=0, train_loss_step=9.06e+3, train_loss_epoch=9.06e+3, val_loss=5.74e+3]\n",
      "Epoch 999:  50%|█████     | 1/2 [00:00<00:00, 22.96it/s, loss=8.5e+03, v_num=0, train_loss_step=8.59e+3, train_loss_epoch=7.97e+3, val_loss=5.74e+3] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 999: 100%|██████████| 2/2 [00:00<00:00, 38.22it/s, loss=8.5e+03, v_num=0, train_loss_step=8.59e+3, train_loss_epoch=7.97e+3, val_loss=5.81e+3]\n",
      "Epoch 999: 100%|██████████| 2/2 [00:00<00:00, 35.17it/s, loss=8.5e+03, v_num=0, train_loss_step=8.59e+3, train_loss_epoch=8.59e+3, val_loss=5.81e+3]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]        \n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.82e+4, train_loss_epoch=4.82e+4]        \n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]        \n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 53.03it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]\n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 50.00it/s, loss=4.73e+04, v_num=0, train_loss_step=4.56e+4, train_loss_epoch=4.56e+4]\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.56e+4, train_loss_epoch=4.56e+4]        \n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.86e+4, train_loss_epoch=4.86e+4]        \n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4]        \n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]        \n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.81e+4, train_loss_epoch=4.81e+4]        \n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]        \n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00, 53.10it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]        \n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.64e+4, train_loss_epoch=4.64e+4]        \n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4]        \n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.63e+4, train_loss_epoch=4.63e+4]         \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.66e+4, train_loss_epoch=4.66e+4]         \n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]        \n",
      "Epoch 99:  50%|█████     | 1/2 [00:00<00:00, 54.98it/s, loss=4.7e+04, v_num=0, train_loss_step=4.59e+4, train_loss_epoch=4.73e+4] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 2/2 [00:00<00:00, 76.40it/s, loss=4.7e+04, v_num=0, train_loss_step=4.59e+4, train_loss_epoch=4.73e+4, val_loss=44165.5]\n",
      "Epoch 100:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.59e+4, train_loss_epoch=4.59e+4, val_loss=44165.5]       \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=44165.5]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.65e+4, train_loss_epoch=4.65e+4, val_loss=44165.5]         \n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=44165.5]        \n",
      "Epoch 121:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=44165.5]        \n",
      "Epoch 126:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.64e+4, train_loss_epoch=4.64e+4, val_loss=44165.5]        \n",
      "Epoch 132:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.71e+04, v_num=0, train_loss_step=4.84e+4, train_loss_epoch=4.84e+4, val_loss=44165.5]        \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=44165.5]         \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=44165.5]        \n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.53e+4, train_loss_epoch=4.53e+4, val_loss=44165.5]         \n",
      "Epoch 152:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.7e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4, val_loss=44165.5]         \n",
      "Epoch 157:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.69e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=44165.5]        \n",
      "Epoch 162:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.68e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=44165.5]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.67e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=44165.5]        \n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.66e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=44165.5]        \n",
      "Epoch 177:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.66e+04, v_num=0, train_loss_step=4.6e+4, train_loss_epoch=4.6e+4, val_loss=44165.5]          \n",
      "Epoch 182:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.67e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=44165.5]        \n",
      "Epoch 187:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.69e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=44165.5]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.68e+04, v_num=0, train_loss_step=4.58e+4, train_loss_epoch=4.58e+4, val_loss=44165.5]        \n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.67e+04, v_num=0, train_loss_step=4.61e+4, train_loss_epoch=4.61e+4, val_loss=44165.5]        \n",
      "Epoch 199:  50%|█████     | 1/2 [00:00<00:00, 54.34it/s, loss=4.65e+04, v_num=0, train_loss_step=4.59e+4, train_loss_epoch=4.59e+4, val_loss=44165.5]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 75.00it/s, loss=4.65e+04, v_num=0, train_loss_step=4.59e+4, train_loss_epoch=4.59e+4, val_loss=4.33e+4]\n",
      "Epoch 202:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.64e+04, v_num=0, train_loss_step=4.66e+4, train_loss_epoch=4.66e+4, val_loss=4.33e+4]        \n",
      "Epoch 206: 100%|██████████| 1/1 [00:00<00:00, 50.52it/s, loss=4.61e+04, v_num=0, train_loss_step=4.56e+4, train_loss_epoch=4.56e+4, val_loss=4.33e+4]\n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.61e+04, v_num=0, train_loss_step=4.56e+4, train_loss_epoch=4.56e+4, val_loss=4.33e+4]        \n",
      "Epoch 207:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.61e+04, v_num=0, train_loss_step=4.56e+4, train_loss_epoch=4.56e+4, val_loss=4.33e+4]\n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.61e+04, v_num=0, train_loss_step=4.59e+4, train_loss_epoch=4.59e+4, val_loss=4.33e+4]        \n",
      "Epoch 216:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.61e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.33e+4]        \n",
      "Epoch 221:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.6e+04, v_num=0, train_loss_step=4.51e+4, train_loss_epoch=4.51e+4, val_loss=4.33e+4]         \n",
      "Epoch 226:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.59e+04, v_num=0, train_loss_step=4.52e+4, train_loss_epoch=4.52e+4, val_loss=4.33e+4]        \n",
      "Epoch 231: 100%|██████████| 1/1 [00:00<00:00, 50.49it/s, loss=4.58e+04, v_num=0, train_loss_step=4.54e+4, train_loss_epoch=4.59e+4, val_loss=4.33e+4]\n",
      "Epoch 232:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.58e+04, v_num=0, train_loss_step=4.54e+4, train_loss_epoch=4.54e+4, val_loss=4.33e+4]        \n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.56e+04, v_num=0, train_loss_step=4.52e+4, train_loss_epoch=4.52e+4, val_loss=4.33e+4]        \n",
      "Epoch 241:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.55e+04, v_num=0, train_loss_step=4.62e+4, train_loss_epoch=4.62e+4, val_loss=4.33e+4]        \n",
      "Epoch 246:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.55e+04, v_num=0, train_loss_step=4.44e+4, train_loss_epoch=4.44e+4, val_loss=4.33e+4]        \n",
      "Epoch 251:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.54e+04, v_num=0, train_loss_step=4.6e+4, train_loss_epoch=4.6e+4, val_loss=4.33e+4]          \n",
      "Epoch 255: 100%|██████████| 1/1 [00:00<00:00, 53.08it/s, loss=4.55e+04, v_num=0, train_loss_step=4.47e+4, train_loss_epoch=4.47e+4, val_loss=4.33e+4]\n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.55e+04, v_num=0, train_loss_step=4.47e+4, train_loss_epoch=4.47e+4, val_loss=4.33e+4]        \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.57e+04, v_num=0, train_loss_step=4.58e+4, train_loss_epoch=4.58e+4, val_loss=4.33e+4]        \n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 52.88it/s, loss=4.57e+04, v_num=0, train_loss_step=4.64e+4, train_loss_epoch=4.64e+4, val_loss=4.33e+4]\n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 50.49it/s, loss=4.57e+04, v_num=0, train_loss_step=4.54e+4, train_loss_epoch=4.54e+4, val_loss=4.33e+4]\n",
      "Epoch 266:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.57e+04, v_num=0, train_loss_step=4.54e+4, train_loss_epoch=4.54e+4, val_loss=4.33e+4]        \n",
      "Epoch 271:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.56e+04, v_num=0, train_loss_step=4.57e+4, train_loss_epoch=4.57e+4, val_loss=4.33e+4]        \n",
      "Epoch 276:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.55e+04, v_num=0, train_loss_step=4.55e+4, train_loss_epoch=4.55e+4, val_loss=4.33e+4]        \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.52e+04, v_num=0, train_loss_step=4.39e+4, train_loss_epoch=4.39e+4, val_loss=4.33e+4]        \n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.48e+04, v_num=0, train_loss_step=4.47e+4, train_loss_epoch=4.47e+4, val_loss=4.33e+4]        \n",
      "Epoch 291:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.47e+04, v_num=0, train_loss_step=4.48e+4, train_loss_epoch=4.48e+4, val_loss=4.33e+4]        \n",
      "Epoch 295: 100%|██████████| 1/1 [00:00<00:00, 52.12it/s, loss=4.47e+04, v_num=0, train_loss_step=4.48e+4, train_loss_epoch=4.48e+4, val_loss=4.33e+4]\n",
      "Epoch 296:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.47e+04, v_num=0, train_loss_step=4.48e+4, train_loss_epoch=4.48e+4, val_loss=4.33e+4]        \n",
      "Epoch 299:  50%|█████     | 1/2 [00:00<00:00, 55.64it/s, loss=4.47e+04, v_num=0, train_loss_step=4.43e+4, train_loss_epoch=4.56e+4, val_loss=4.33e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 77.35it/s, loss=4.47e+04, v_num=0, train_loss_step=4.43e+4, train_loss_epoch=4.56e+4, val_loss=4.18e+4]\n",
      "Epoch 300:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.47e+04, v_num=0, train_loss_step=4.43e+4, train_loss_epoch=4.43e+4, val_loss=4.18e+4]        \n",
      "Epoch 305:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.48e+04, v_num=0, train_loss_step=4.53e+4, train_loss_epoch=4.53e+4, val_loss=4.18e+4]        \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.48e+04, v_num=0, train_loss_step=4.37e+4, train_loss_epoch=4.37e+4, val_loss=4.18e+4]        \n",
      "Epoch 315:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.47e+04, v_num=0, train_loss_step=4.53e+4, train_loss_epoch=4.53e+4, val_loss=4.18e+4]        \n",
      "Epoch 320:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.47e+04, v_num=0, train_loss_step=4.47e+4, train_loss_epoch=4.47e+4, val_loss=4.18e+4]        \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.46e+04, v_num=0, train_loss_step=4.45e+4, train_loss_epoch=4.45e+4, val_loss=4.18e+4]        \n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.43e+04, v_num=0, train_loss_step=4.32e+4, train_loss_epoch=4.32e+4, val_loss=4.18e+4]        \n",
      "Epoch 335:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.44e+04, v_num=0, train_loss_step=4.55e+4, train_loss_epoch=4.55e+4, val_loss=4.18e+4]        \n",
      "Epoch 341:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.43e+04, v_num=0, train_loss_step=4.51e+4, train_loss_epoch=4.51e+4, val_loss=4.18e+4]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.41e+04, v_num=0, train_loss_step=4.39e+4, train_loss_epoch=4.39e+4, val_loss=4.18e+4]        \n",
      "Epoch 345: 100%|██████████| 1/1 [00:00<00:00, 54.02it/s, loss=4.41e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.39e+4, val_loss=4.18e+4]\n",
      "Epoch 345: 100%|██████████| 1/1 [00:00<00:00, 52.53it/s, loss=4.41e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.31e+4, val_loss=4.18e+4]\n",
      "Epoch 346:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.41e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.31e+4, val_loss=4.18e+4]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.42e+04, v_num=0, train_loss_step=4.46e+4, train_loss_epoch=4.46e+4, val_loss=4.18e+4]        \n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.39e+04, v_num=0, train_loss_step=4.4e+4, train_loss_epoch=4.4e+4, val_loss=4.18e+4]          \n",
      "Epoch 361:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.38e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.31e+4, val_loss=4.18e+4]        \n",
      "Epoch 367:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.37e+04, v_num=0, train_loss_step=4.34e+4, train_loss_epoch=4.34e+4, val_loss=4.18e+4]        \n",
      "Epoch 372:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.34e+04, v_num=0, train_loss_step=4.39e+4, train_loss_epoch=4.39e+4, val_loss=4.18e+4]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.32e+04, v_num=0, train_loss_step=4.37e+4, train_loss_epoch=4.37e+4, val_loss=4.18e+4]        \n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.32e+04, v_num=0, train_loss_step=4.22e+4, train_loss_epoch=4.22e+4, val_loss=4.18e+4]        \n",
      "Epoch 387:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.32e+04, v_num=0, train_loss_step=4.3e+4, train_loss_epoch=4.3e+4, val_loss=4.18e+4]          \n",
      "Epoch 392:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.31e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.31e+4, val_loss=4.18e+4]        \n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.29e+04, v_num=0, train_loss_step=4.25e+4, train_loss_epoch=4.25e+4, val_loss=4.18e+4]        \n",
      "Epoch 399:  50%|█████     | 1/2 [00:00<00:00, 56.31it/s, loss=4.28e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.34e+4, val_loss=4.18e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 399: 100%|██████████| 2/2 [00:00<00:00, 77.47it/s, loss=4.28e+04, v_num=0, train_loss_step=4.31e+4, train_loss_epoch=4.34e+4, val_loss=3.96e+4]\n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.28e+04, v_num=0, train_loss_step=4.26e+4, train_loss_epoch=4.26e+4, val_loss=3.96e+4]        \n",
      "Epoch 407:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.27e+04, v_num=0, train_loss_step=4.25e+4, train_loss_epoch=4.25e+4, val_loss=3.96e+4]        \n",
      "Epoch 413:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.24e+04, v_num=0, train_loss_step=4.25e+4, train_loss_epoch=4.25e+4, val_loss=3.96e+4]        \n",
      "Epoch 418:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.23e+04, v_num=0, train_loss_step=4.24e+4, train_loss_epoch=4.24e+4, val_loss=3.96e+4]        \n",
      "Epoch 423:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.23e+04, v_num=0, train_loss_step=4.32e+4, train_loss_epoch=4.32e+4, val_loss=3.96e+4]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.22e+04, v_num=0, train_loss_step=4.13e+4, train_loss_epoch=4.13e+4, val_loss=3.96e+4]        \n",
      "Epoch 433:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.2e+04, v_num=0, train_loss_step=4.13e+4, train_loss_epoch=4.13e+4, val_loss=3.96e+4]         \n",
      "Epoch 438:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.19e+04, v_num=0, train_loss_step=4.13e+4, train_loss_epoch=4.13e+4, val_loss=3.96e+4]        \n",
      "Epoch 443:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.16e+04, v_num=0, train_loss_step=4.14e+4, train_loss_epoch=4.14e+4, val_loss=3.96e+4]        \n",
      "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.17e+04, v_num=0, train_loss_step=4.04e+4, train_loss_epoch=4.04e+4, val_loss=3.96e+4]        \n",
      "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.15e+04, v_num=0, train_loss_step=4.15e+4, train_loss_epoch=4.15e+4, val_loss=3.96e+4]        \n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.13e+04, v_num=0, train_loss_step=4.08e+4, train_loss_epoch=4.08e+4, val_loss=3.96e+4]        \n",
      "Epoch 464:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.12e+04, v_num=0, train_loss_step=4.15e+4, train_loss_epoch=4.15e+4, val_loss=3.96e+4]        \n",
      "Epoch 470:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.09e+04, v_num=0, train_loss_step=4.09e+4, train_loss_epoch=4.09e+4, val_loss=3.96e+4]        \n",
      "Epoch 475:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.07e+04, v_num=0, train_loss_step=4.14e+4, train_loss_epoch=4.14e+4, val_loss=3.96e+4]        \n",
      "Epoch 480:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.06e+04, v_num=0, train_loss_step=4.03e+4, train_loss_epoch=4.03e+4, val_loss=3.96e+4]        \n",
      "Epoch 485:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.04e+04, v_num=0, train_loss_step=4.04e+4, train_loss_epoch=4.04e+4, val_loss=3.96e+4]        \n",
      "Epoch 490:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.03e+04, v_num=0, train_loss_step=3.96e+4, train_loss_epoch=3.96e+4, val_loss=3.96e+4]        \n",
      "Epoch 496:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.02e+04, v_num=0, train_loss_step=3.97e+4, train_loss_epoch=3.97e+4, val_loss=3.96e+4]        \n",
      "Epoch 499:  50%|█████     | 1/2 [00:00<00:00, 52.76it/s, loss=4.01e+04, v_num=0, train_loss_step=3.96e+4, train_loss_epoch=3.94e+4, val_loss=3.96e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00, 71.81it/s, loss=4.01e+04, v_num=0, train_loss_step=3.96e+4, train_loss_epoch=3.94e+4, val_loss=3.69e+4]\n",
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00, 63.67it/s, loss=4.01e+04, v_num=0, train_loss_step=3.96e+4, train_loss_epoch=3.96e+4, val_loss=3.69e+4]\n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             \n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 25.14it/s, loss=4.77e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 26.12it/s, loss=4.73e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 24.31it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.81e+4, train_loss_epoch=4.81e+4]        \n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]        \n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]        \n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 24.76it/s, loss=4.74e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 24.48it/s, loss=4.74e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 25.74it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]        \n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4]        \n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 26.03it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]\n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 26.42it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]\n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 25.46it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]        \n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4]          \n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 26.43it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]\n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 24.92it/s, loss=4.74e+04, v_num=0, train_loss_step=4.65e+4, train_loss_epoch=4.65e+4]\n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 26.37it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]        \n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 26.45it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]\n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 26.42it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]        \n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4]\n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]\n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 26.34it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]        \n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]        \n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 26.42it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 40: 100%|██████████| 1/1 [00:00<00:00, 26.43it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4]\n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 25.31it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4]  \n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4]        \n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]        \n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.81e+4, train_loss_epoch=4.81e+4]        \n",
      "Epoch 46: 100%|██████████| 1/1 [00:00<00:00, 24.88it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]\n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 26.27it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]\n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 25.41it/s, loss=4.75e+04, v_num=0, train_loss_step=47881.0, train_loss_epoch=47881.0]\n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 11.09it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4]\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4]        \n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]        \n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]        \n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00, 24.92it/s, loss=4.75e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]\n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00, 24.74it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 55: 100%|██████████| 1/1 [00:00<00:00, 24.68it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.65e+4]\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00, 11.97it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4]        \n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]        \n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]\n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00, 26.33it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]\n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00, 25.52it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4]        \n",
      "Epoch 66: 100%|██████████| 1/1 [00:00<00:00, 26.36it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4]  \n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]\n",
      "Epoch 68: 100%|██████████| 1/1 [00:00<00:00, 26.39it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4]\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 72: 100%|██████████| 1/1 [00:00<00:00, 26.43it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]\n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00, 25.63it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]\n",
      "Epoch 74: 100%|██████████| 1/1 [00:00<00:00, 25.49it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]        \n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]        \n",
      "Epoch 78: 100%|██████████| 1/1 [00:00<00:00, 26.45it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]\n",
      "Epoch 79: 100%|██████████| 1/1 [00:00<00:00, 26.09it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]\n",
      "Epoch 80: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4]\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 84: 100%|██████████| 1/1 [00:00<00:00, 26.10it/s, loss=4.75e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4]  \n",
      "Epoch 85: 100%|██████████| 1/1 [00:00<00:00, 26.47it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]\n",
      "Epoch 86: 100%|██████████| 1/1 [00:00<00:00, 24.60it/s, loss=4.75e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4]\n",
      "Epoch 87: 100%|██████████| 1/1 [00:00<00:00, 24.68it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4]\n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4]        \n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4]        \n",
      "Epoch 91: 100%|██████████| 1/1 [00:00<00:00, 24.57it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4]  \n",
      "Epoch 92: 100%|██████████| 1/1 [00:00<00:00, 24.48it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4]\n",
      "Epoch 93: 100%|██████████| 1/1 [00:00<00:00, 24.26it/s, loss=4.73e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4]\n",
      "Epoch 94: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4]\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4]        \n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4]        \n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4]        \n",
      "Epoch 99:  50%|█████     | 1/2 [00:00<00:00, 24.90it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4]\n",
      "Epoch 99:  50%|█████     | 1/2 [00:00<00:00, 11.54it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.69e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99: 100%|██████████| 2/2 [00:00<00:00, 21.05it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 100: 100%|██████████| 1/1 [00:00<00:00, 26.42it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 101: 100%|██████████| 1/1 [00:00<00:00, 25.36it/s, loss=4.74e+04, v_num=0, train_loss_step=4.81e+4, train_loss_epoch=4.81e+4, val_loss=4.44e+4]\n",
      "Epoch 102: 100%|██████████| 1/1 [00:00<00:00, 25.14it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 104:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 105:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 106: 100%|██████████| 1/1 [00:00<00:00, 26.18it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 107: 100%|██████████| 1/1 [00:00<00:00, 26.03it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 108: 100%|██████████| 1/1 [00:00<00:00, 26.36it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 110:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 111:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 112: 100%|██████████| 1/1 [00:00<00:00, 26.27it/s, loss=4.74e+04, v_num=0, train_loss_step=4.63e+4, train_loss_epoch=4.63e+4, val_loss=4.44e+4]\n",
      "Epoch 113: 100%|██████████| 1/1 [00:00<00:00, 26.26it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 114: 100%|██████████| 1/1 [00:00<00:00, 25.36it/s, loss=4.74e+04, v_num=0, train_loss_step=4.64e+4, train_loss_epoch=4.64e+4, val_loss=4.44e+4]\n",
      "Epoch 116:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 117:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 118: 100%|██████████| 1/1 [00:00<00:00, 26.24it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 119: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 120: 100%|██████████| 1/1 [00:00<00:00, 26.19it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 122:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]        \n",
      "Epoch 123:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 124:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 124: 100%|██████████| 1/1 [00:00<00:00, 26.23it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 125: 100%|██████████| 1/1 [00:00<00:00, 26.28it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 126: 100%|██████████| 1/1 [00:00<00:00, 25.50it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 128:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 129:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 130:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]        \n",
      "Epoch 131: 100%|██████████| 1/1 [00:00<00:00, 26.08it/s, loss=4.73e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]\n",
      "Epoch 132: 100%|██████████| 1/1 [00:00<00:00, 25.98it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 133: 100%|██████████| 1/1 [00:00<00:00, 24.23it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 135:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 136:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 137:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 138: 100%|██████████| 1/1 [00:00<00:00, 25.52it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 139: 100%|██████████| 1/1 [00:00<00:00, 26.27it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 141:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 142:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]        \n",
      "Epoch 143:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 144: 100%|██████████| 1/1 [00:00<00:00, 26.27it/s, loss=4.72e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 145: 100%|██████████| 1/1 [00:00<00:00, 25.32it/s, loss=4.72e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 26.32it/s, loss=4.72e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 146: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 147:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 148:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 149:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 150: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s, loss=4.73e+04, v_num=0, train_loss_step=4.8e+4, train_loss_epoch=4.8e+4, val_loss=4.44e+4]  \n",
      "Epoch 151: 100%|██████████| 1/1 [00:00<00:00, 26.27it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]\n",
      "Epoch 152: 100%|██████████| 1/1 [00:00<00:00, 25.30it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 154:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 155:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 156: 100%|██████████| 1/1 [00:00<00:00, 26.21it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 157: 100%|██████████| 1/1 [00:00<00:00, 25.99it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 158: 100%|██████████| 1/1 [00:00<00:00, 26.26it/s, loss=4.73e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 160:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 161:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 162: 100%|██████████| 1/1 [00:00<00:00, 26.16it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 163: 100%|██████████| 1/1 [00:00<00:00, 26.24it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 164: 100%|██████████| 1/1 [00:00<00:00, 24.50it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 166:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 167:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4, val_loss=4.44e+4]        \n",
      "Epoch 168:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 169: 100%|██████████| 1/1 [00:00<00:00, 25.31it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 170: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 172:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 173:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 174:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 175: 100%|██████████| 1/1 [00:00<00:00, 25.93it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 176: 100%|██████████| 1/1 [00:00<00:00, 25.01it/s, loss=4.73e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]\n",
      "Epoch 177: 100%|██████████| 1/1 [00:00<00:00, 26.29it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 179:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]        \n",
      "Epoch 180:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 181: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 182: 100%|██████████| 1/1 [00:00<00:00, 26.28it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 183: 100%|██████████| 1/1 [00:00<00:00, 25.29it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 185:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 186:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 187: 100%|██████████| 1/1 [00:00<00:00, 26.18it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 188: 100%|██████████| 1/1 [00:00<00:00, 25.25it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 189: 100%|██████████| 1/1 [00:00<00:00, 26.20it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 191:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4, val_loss=4.44e+4]        \n",
      "Epoch 192:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 193:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]        \n",
      "Epoch 193: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 194: 100%|██████████| 1/1 [00:00<00:00, 26.29it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 195: 100%|██████████| 1/1 [00:00<00:00, 25.50it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 197:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 198:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 199:   0%|          | 0/2 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 199:  50%|█████     | 1/2 [00:00<00:00, 11.80it/s, loss=4.72e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 21.53it/s, loss=4.72e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 200: 100%|██████████| 1/1 [00:00<00:00, 26.11it/s, loss=4.72e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 201: 100%|██████████| 1/1 [00:00<00:00, 26.37it/s, loss=4.72e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 202: 100%|██████████| 1/1 [00:00<00:00, 25.42it/s, loss=4.72e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 204:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 205:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 206:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 207: 100%|██████████| 1/1 [00:00<00:00, 25.97it/s, loss=4.72e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 208: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s, loss=4.72e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 25.46it/s, loss=4.72e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 209: 100%|██████████| 1/1 [00:00<00:00, 11.61it/s, loss=4.72e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 210:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 211:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 212:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]        \n",
      "Epoch 213: 100%|██████████| 1/1 [00:00<00:00, 26.27it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 214: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s, loss=4.73e+04, v_num=0, train_loss_step=4.81e+4, train_loss_epoch=4.81e+4, val_loss=4.44e+4]\n",
      "Epoch 215: 100%|██████████| 1/1 [00:00<00:00, 26.24it/s, loss=4.74e+04, v_num=0, train_loss_step=4.8e+4, train_loss_epoch=4.8e+4, val_loss=4.44e+4]  \n",
      "Epoch 217:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 218:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 219: 100%|██████████| 1/1 [00:00<00:00, 26.05it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 220: 100%|██████████| 1/1 [00:00<00:00, 26.14it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 221: 100%|██████████| 1/1 [00:00<00:00, 25.66it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 223:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]        \n",
      "Epoch 224:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 225: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 226: 100%|██████████| 1/1 [00:00<00:00, 26.12it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 227: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 229:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 230:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]        \n",
      "Epoch 231:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 232: 100%|██████████| 1/1 [00:00<00:00, 26.13it/s, loss=4.74e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]\n",
      "Epoch 233: 100%|██████████| 1/1 [00:00<00:00, 25.38it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 234: 100%|██████████| 1/1 [00:00<00:00, 11.98it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]\n",
      "Epoch 234:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 235:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 236:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 237:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 238: 100%|██████████| 1/1 [00:00<00:00, 26.02it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 239: 100%|██████████| 1/1 [00:00<00:00, 26.23it/s, loss=4.73e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4, val_loss=4.44e+4]\n",
      "Epoch 240: 100%|██████████| 1/1 [00:00<00:00, 25.27it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 242:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 243:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]        \n",
      "Epoch 244: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s, loss=4.73e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4, val_loss=4.44e+4]\n",
      "Epoch 245: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 246: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 248:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 249:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 250: 100%|██████████| 1/1 [00:00<00:00, 26.36it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 251: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s, loss=4.73e+04, v_num=0, train_loss_step=4.82e+4, train_loss_epoch=4.82e+4, val_loss=4.44e+4]\n",
      "Epoch 252: 100%|██████████| 1/1 [00:00<00:00, 25.38it/s, loss=4.73e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 254:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 255:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 256:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 257: 100%|██████████| 1/1 [00:00<00:00, 26.19it/s, loss=4.73e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]\n",
      "Epoch 258: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s, loss=4.72e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 259: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s, loss=4.73e+04, v_num=0, train_loss_step=4.8e+4, train_loss_epoch=4.8e+4, val_loss=4.44e+4]  \n",
      "Epoch 261:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 262:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 263: 100%|██████████| 1/1 [00:00<00:00, 26.21it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 264: 100%|██████████| 1/1 [00:00<00:00, 25.32it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 265: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 267:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 268:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 269: 100%|██████████| 1/1 [00:00<00:00, 26.04it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 270: 100%|██████████| 1/1 [00:00<00:00, 26.23it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 271: 100%|██████████| 1/1 [00:00<00:00, 24.67it/s, loss=4.73e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 273:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 274:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 275:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 276: 100%|██████████| 1/1 [00:00<00:00, 25.98it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 277: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=4.73e+04, v_num=0, train_loss_step=4.66e+4, train_loss_epoch=4.66e+4, val_loss=4.44e+4]\n",
      "Epoch 278: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 278: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 279:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 280:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]        \n",
      "Epoch 281:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 282: 100%|██████████| 1/1 [00:00<00:00, 26.05it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 283: 100%|██████████| 1/1 [00:00<00:00, 24.46it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 284: 100%|██████████| 1/1 [00:00<00:00, 24.41it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 286:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 287:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 288:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 289: 100%|██████████| 1/1 [00:00<00:00, 26.28it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 290: 100%|██████████| 1/1 [00:00<00:00, 26.10it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 291: 100%|██████████| 1/1 [00:00<00:00, 24.37it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 292: 100%|██████████| 1/1 [00:00<00:00, 24.57it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 294:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 295:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 296: 100%|██████████| 1/1 [00:00<00:00, 24.53it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 297: 100%|██████████| 1/1 [00:00<00:00, 24.78it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 298: 100%|██████████| 1/1 [00:00<00:00, 26.21it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 299:  50%|█████     | 1/2 [00:00<00:00, 25.22it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 299:  50%|█████     | 1/2 [00:00<00:00, 11.74it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 21.45it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]\n",
      "Epoch 301:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 302:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 303:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 304: 100%|██████████| 1/1 [00:00<00:00, 25.29it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 305: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 306: 100%|██████████| 1/1 [00:00<00:00, 24.33it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 307: 100%|██████████| 1/1 [00:00<00:00, 24.35it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 309:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 310:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 311: 100%|██████████| 1/1 [00:00<00:00, 25.53it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 312: 100%|██████████| 1/1 [00:00<00:00, 24.54it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 313: 100%|██████████| 1/1 [00:00<00:00, 24.33it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 314: 100%|██████████| 1/1 [00:00<00:00, 26.15it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 316:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 317:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 318:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 319: 100%|██████████| 1/1 [00:00<00:00, 26.23it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 320: 100%|██████████| 1/1 [00:00<00:00, 25.48it/s, loss=4.74e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 321: 100%|██████████| 1/1 [00:00<00:00, 24.25it/s, loss=4.74e+04, v_num=0, train_loss_step=4.82e+4, train_loss_epoch=4.82e+4, val_loss=4.44e+4]\n",
      "Epoch 323:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 324:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 325:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 326: 100%|██████████| 1/1 [00:00<00:00, 24.54it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 327: 100%|██████████| 1/1 [00:00<00:00, 24.50it/s, loss=4.75e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]\n",
      "Epoch 328: 100%|██████████| 1/1 [00:00<00:00, 24.77it/s, loss=4.75e+04, v_num=0, train_loss_step=4.8e+4, train_loss_epoch=4.8e+4, val_loss=4.44e+4]  \n",
      "Epoch 329: 100%|██████████| 1/1 [00:00<00:00, 25.34it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 331:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]        \n",
      "Epoch 332:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 333: 100%|██████████| 1/1 [00:00<00:00, 26.08it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 334: 100%|██████████| 1/1 [00:00<00:00, 25.18it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 335: 100%|██████████| 1/1 [00:00<00:00, 26.19it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 337:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 338:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 339:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 340: 100%|██████████| 1/1 [00:00<00:00, 26.32it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 341: 100%|██████████| 1/1 [00:00<00:00, 24.50it/s, loss=4.75e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 342: 100%|██████████| 1/1 [00:00<00:00, 25.96it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 344:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 345:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 346: 100%|██████████| 1/1 [00:00<00:00, 26.14it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 347: 100%|██████████| 1/1 [00:00<00:00, 25.48it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 348: 100%|██████████| 1/1 [00:00<00:00, 25.38it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 350:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 351:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 352:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]        \n",
      "Epoch 352: 100%|██████████| 1/1 [00:00<00:00, 26.16it/s, loss=4.75e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 353: 100%|██████████| 1/1 [00:00<00:00, 24.67it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 354: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 355: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s, loss=4.75e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 355: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s, loss=4.75e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 356:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 357:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 358:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=47308.0, train_loss_epoch=47308.0, val_loss=4.44e+4]        \n",
      "Epoch 359: 100%|██████████| 1/1 [00:00<00:00, 26.12it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 360: 100%|██████████| 1/1 [00:00<00:00, 24.29it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 361: 100%|██████████| 1/1 [00:00<00:00, 26.09it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 363:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]        \n",
      "Epoch 364:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]        \n",
      "Epoch 365: 100%|██████████| 1/1 [00:00<00:00, 25.26it/s, loss=4.74e+04, v_num=0, train_loss_step=4.64e+4, train_loss_epoch=4.64e+4, val_loss=4.44e+4]\n",
      "Epoch 366: 100%|██████████| 1/1 [00:00<00:00, 25.25it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 367: 100%|██████████| 1/1 [00:00<00:00, 24.79it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 369:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 370:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 371:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 372: 100%|██████████| 1/1 [00:00<00:00, 24.67it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 373: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 374: 100%|██████████| 1/1 [00:00<00:00, 25.30it/s, loss=4.74e+04, v_num=0, train_loss_step=4.82e+4, train_loss_epoch=4.82e+4, val_loss=4.44e+4]\n",
      "Epoch 376:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 377:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]        \n",
      "Epoch 378: 100%|██████████| 1/1 [00:00<00:00, 26.17it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 379: 100%|██████████| 1/1 [00:00<00:00, 24.48it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 380: 100%|██████████| 1/1 [00:00<00:00, 26.17it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 382:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 383:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 384:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 385: 100%|██████████| 1/1 [00:00<00:00, 25.22it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 386: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 387: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=4.75e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 389:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]        \n",
      "Epoch 390:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]        \n",
      "Epoch 391: 100%|██████████| 1/1 [00:00<00:00, 24.94it/s, loss=4.75e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 392: 100%|██████████| 1/1 [00:00<00:00, 26.19it/s, loss=4.75e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 393: 100%|██████████| 1/1 [00:00<00:00, 25.41it/s, loss=4.75e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]\n",
      "Epoch 395:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 396:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.75e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 397:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.65e+4, train_loss_epoch=4.65e+4, val_loss=4.44e+4]        \n",
      "Epoch 398: 100%|██████████| 1/1 [00:00<00:00, 24.52it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 399:  50%|█████     | 1/2 [00:00<00:00, 26.13it/s, loss=4.74e+04, v_num=0, train_loss_step=4.67e+4, train_loss_epoch=4.67e+4, val_loss=4.44e+4]\n",
      "Epoch 399:  50%|█████     | 1/2 [00:00<00:00, 11.90it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.67e+4, val_loss=4.44e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 399: 100%|██████████| 2/2 [00:00<00:00, 21.63it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.67e+4, val_loss=4.44e+4]\n",
      "Epoch 400: 100%|██████████| 1/1 [00:00<00:00, 24.76it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 402:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 403:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 404:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 405: 100%|██████████| 1/1 [00:00<00:00, 25.17it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 406: 100%|██████████| 1/1 [00:00<00:00, 26.06it/s, loss=4.74e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]\n",
      "Epoch 407: 100%|██████████| 1/1 [00:00<00:00, 25.47it/s, loss=4.73e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 407: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 408:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 409:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 410:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 411: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 412: 100%|██████████| 1/1 [00:00<00:00, 24.64it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 413: 100%|██████████| 1/1 [00:00<00:00, 26.20it/s, loss=4.73e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]\n",
      "Epoch 415:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]        \n",
      "Epoch 416:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.72e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 417: 100%|██████████| 1/1 [00:00<00:00, 25.31it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 418: 100%|██████████| 1/1 [00:00<00:00, 26.11it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 419: 100%|██████████| 1/1 [00:00<00:00, 25.33it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 421:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]        \n",
      "Epoch 422:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 423: 100%|██████████| 1/1 [00:00<00:00, 25.21it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 424: 100%|██████████| 1/1 [00:00<00:00, 24.80it/s, loss=4.72e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]\n",
      "Epoch 425: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=4.72e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 427:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 428:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 429:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 430: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 431: 100%|██████████| 1/1 [00:00<00:00, 25.17it/s, loss=4.74e+04, v_num=0, train_loss_step=47507.0, train_loss_epoch=47507.0, val_loss=4.44e+4]\n",
      "Epoch 432: 100%|██████████| 1/1 [00:00<00:00, 26.09it/s, loss=4.73e+04, v_num=0, train_loss_step=47231.5, train_loss_epoch=47231.5, val_loss=4.44e+4]\n",
      "Epoch 434:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]        \n",
      "Epoch 435:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 436: 100%|██████████| 1/1 [00:00<00:00, 24.93it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 437: 100%|██████████| 1/1 [00:00<00:00, 26.15it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 438: 100%|██████████| 1/1 [00:00<00:00, 25.13it/s, loss=4.73e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]\n",
      "Epoch 440:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 441:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 442:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 443: 100%|██████████| 1/1 [00:00<00:00, 25.30it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 444: 100%|██████████| 1/1 [00:00<00:00, 26.13it/s, loss=4.74e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]\n",
      "Epoch 445: 100%|██████████| 1/1 [00:00<00:00, 26.05it/s, loss=4.74e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]\n",
      "Epoch 447:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.68e+4, train_loss_epoch=4.68e+4, val_loss=4.44e+4]        \n",
      "Epoch 448:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 449: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 450: 100%|██████████| 1/1 [00:00<00:00, 25.27it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 451: 100%|██████████| 1/1 [00:00<00:00, 26.09it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 453:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 454:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.79e+4, train_loss_epoch=4.79e+4, val_loss=4.44e+4]        \n",
      "Epoch 455:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.71e+4, train_loss_epoch=4.71e+4, val_loss=4.44e+4]        \n",
      "Epoch 456: 100%|██████████| 1/1 [00:00<00:00, 26.12it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 457: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 458: 100%|██████████| 1/1 [00:00<00:00, 26.10it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 458: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]\n",
      "Epoch 459:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]        \n",
      "Epoch 460:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 461:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 462: 100%|██████████| 1/1 [00:00<00:00, 26.16it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 463: 100%|██████████| 1/1 [00:00<00:00, 25.97it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]  \n",
      "Epoch 464: 100%|██████████| 1/1 [00:00<00:00, 24.88it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 466:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]        \n",
      "Epoch 467:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 468:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 469: 100%|██████████| 1/1 [00:00<00:00, 25.21it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 470: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 471: 100%|██████████| 1/1 [00:00<00:00, 25.96it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]\n",
      "Epoch 473:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]        \n",
      "Epoch 474:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.64e+4, train_loss_epoch=4.64e+4, val_loss=4.44e+4]        \n",
      "Epoch 475: 100%|██████████| 1/1 [00:00<00:00, 26.12it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 476: 100%|██████████| 1/1 [00:00<00:00, 25.18it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]\n",
      "Epoch 477: 100%|██████████| 1/1 [00:00<00:00, 26.06it/s, loss=4.73e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 479:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.7e+4, train_loss_epoch=4.7e+4, val_loss=4.44e+4]          \n",
      "Epoch 480:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 481:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=47213.0, train_loss_epoch=47213.0, val_loss=4.44e+4]        \n",
      "Epoch 481: 100%|██████████| 1/1 [00:00<00:00, 25.98it/s, loss=4.73e+04, v_num=0, train_loss_step=47213.0, train_loss_epoch=47213.0, val_loss=4.44e+4]\n",
      "Epoch 482: 100%|██████████| 1/1 [00:00<00:00, 26.11it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 483: 100%|██████████| 1/1 [00:00<00:00, 25.11it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 485:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 486:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]        \n",
      "Epoch 487:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.73e+04, v_num=0, train_loss_step=4.77e+4, train_loss_epoch=4.77e+4, val_loss=4.44e+4]        \n",
      "Epoch 488: 100%|██████████| 1/1 [00:00<00:00, 26.13it/s, loss=4.73e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 489: 100%|██████████| 1/1 [00:00<00:00, 26.08it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 490: 100%|██████████| 1/1 [00:00<00:00, 25.15it/s, loss=4.73e+04, v_num=0, train_loss_step=4.75e+4, train_loss_epoch=4.75e+4, val_loss=4.44e+4]\n",
      "Epoch 492:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.73e+4, train_loss_epoch=4.73e+4, val_loss=4.44e+4]        \n",
      "Epoch 493:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 494: 100%|██████████| 1/1 [00:00<00:00, 26.11it/s, loss=4.74e+04, v_num=0, train_loss_step=4.76e+4, train_loss_epoch=4.76e+4, val_loss=4.44e+4]\n",
      "Epoch 495: 100%|██████████| 1/1 [00:00<00:00, 24.61it/s, loss=4.74e+04, v_num=0, train_loss_step=4.78e+4, train_loss_epoch=4.78e+4, val_loss=4.44e+4]\n",
      "Epoch 496: 100%|██████████| 1/1 [00:00<00:00, 26.09it/s, loss=4.74e+04, v_num=0, train_loss_step=4.72e+4, train_loss_epoch=4.72e+4, val_loss=4.44e+4]\n",
      "Epoch 498:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.69e+4, train_loss_epoch=4.69e+4, val_loss=4.44e+4]        \n",
      "Epoch 499:   0%|          | 0/2 [00:00<?, ?it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]        \n",
      "Epoch 499:  50%|█████     | 1/2 [00:00<00:00, 11.82it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00, 21.36it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n",
      "Epoch 499: 100%|██████████| 2/2 [00:00<00:00, 20.62it/s, loss=4.74e+04, v_num=0, train_loss_step=4.74e+4, train_loss_epoch=4.74e+4, val_loss=4.44e+4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /mnt/276CE6AF4D37CF1C/proj_kaggle/microbiz/lightning_logs\n",
      "2023-02-03 14:11:39.517436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-03 14:11:40.083842: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-02-03 14:11:40.083990: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-02-03 14:11:40.083997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f684949458d4d6dbcf392e67a82e2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nf = NeuralForecast(\n",
    "    models=models,\n",
    "    freq='H')\n",
    "\n",
    "nf.fit(df=Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9bc62c-23d5-41c7-95cb-42b8aa68cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f651b57507334970b621d9c52d890b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>AutoTFT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ERCOT</th>\n",
       "      <td>2022-10-01 00:00:00</td>\n",
       "      <td>41044.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERCOT</th>\n",
       "      <td>2022-10-01 01:00:00</td>\n",
       "      <td>44394.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERCOT</th>\n",
       "      <td>2022-10-01 02:00:00</td>\n",
       "      <td>45196.335938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERCOT</th>\n",
       "      <td>2022-10-01 03:00:00</td>\n",
       "      <td>45298.542969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ERCOT</th>\n",
       "      <td>2022-10-01 04:00:00</td>\n",
       "      <td>45312.171875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ds       AutoTFT\n",
       "unique_id                                  \n",
       "ERCOT     2022-10-01 00:00:00  41044.187500\n",
       "ERCOT     2022-10-01 01:00:00  44394.914062\n",
       "ERCOT     2022-10-01 02:00:00  45196.335938\n",
       "ERCOT     2022-10-01 03:00:00  45298.542969\n",
       "ERCOT     2022-10-01 04:00:00  45312.171875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat_df = nf.predict()\n",
    "Y_hat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d843a3a-6b5e-4248-b666-27c74d0e4b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 3))\n",
    "plot_df = pd.concat([Y_df.tail(24*5).reset_index(drop=True), Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n",
    "plot_df[['y', 'AutoTFT']].plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('Load [MW]', fontsize=12)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=12)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.legend(prop={'size': 10})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf143ccc-e63a-42a6-9006-487199931736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
